[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Bas Jacobs. I am working as a data scientist in The Netherlands. Before that, I did both my bachelor and master in mathematics. I like solving puzzles, writing code and working on data analysis and machine learning projects.\n\nThis blog was made using quarto."
  },
  {
    "objectID": "post/2024-04-28-dutch-placename-mapper/index.html",
    "href": "post/2024-04-28-dutch-placename-mapper/index.html",
    "title": "Dutch placename mapper",
    "section": "",
    "text": "Inspired by https://placenames.rtwilson.com, I thought I’d try to make such place name visualizations of the Netherlands myself.\nThe Dutch platform PDOK provides open geo-information datasets through webservices and downloads. Here, we use the location API to search for residence locations using Solr-syntax. We also use the gray background map from PDOK, and use leaflet to create interactive maps.\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sf)\n\npdok_query_url &lt;- \"https://api.pdok.nl/bzk/locatieserver/search/v3_1/free?q={location}&fq=type:woonplaats&fq=bron:BAG&fl=id woonplaatsnaam weergavenaam centroide_ll&df=woonplaatsnaam&rows=100\"\npdok_map_url &lt;- \"https://service.pdok.nl/brt/achtergrondkaart/wmts/v2_0/grijs/EPSG:3857/{z}/{x}/{y}.png\"\n\naddLocationMarkers &lt;- function(map, location, fillColor) {\n  url &lt;- URLencode(str_glue(pdok_query_url))\n  res &lt;- jsonlite::read_json(url, simplifyVector = TRUE)\n  \n  df &lt;- bind_rows(res$response$docs) %&gt;% \n    mutate(lng = as.numeric(str_extract(centroide_ll, \"[\\\\d\\\\.]+\\\\s\")),\n           lat = as.numeric(str_extract(centroide_ll, \"\\\\s[\\\\d\\\\.]+\")))\n  \n  map &lt;- addCircleMarkers(map, lng=df$lng, lat=df$lat, popup = df$woonplaatsnaam,\n                          stroke = F, radius = 5, fillOpacity = 1, fillColor = fillColor, color = fillColor)\n  \n  return(map)\n}\n\ncreate_map &lt;- function(markers) {\n  \n  map &lt;- leaflet() %&gt;% \n    setView(zoom=7, lng=5.4, lat=52.2) %&gt;%\n    addTiles(urlTemplate = pdok_map_url,\n             attribution = \"&lt;a href='https://www.pdok.nl/'&gt;PDOK&lt;/a&gt;\",\n             options = tileOptions(minZoom = 6, maxZoom = 18)\n    )\n  \n  for (location in names(markers)) {\n    map &lt;- addLocationMarkers(map, location, markers[[location]])\n  }\n  map &lt;- addLegend(map, position = \"bottomright\",\n                   colors = unname(unlist(markers)),\n                   labels = names(markers),\n                   opacity = 1)\n  return(map)\n}\n\nThis allows us to create maps of common Dutch toponym patterns, such as “loo” (Germanic), “recht” (Roman) or “ga” (Frisian).\n\ncreate_map(list(\n  \"*loo OR *lo OR *le\" = \"blue\",\n  \"*recht OR *richt\" = \"green\",\n  \"*ga\" = \"red\",\n  \"*wolde\" = \"orange\",\n  \"*schot\" = \"black\",\n  \"*wijk\" = \"magenta\"\n))\n\n\n\n\n\nOr places which contain river names:\n\ncreate_map(list(\n  \"*rijn*\" = \"red\",\n  \"*maas*\" = \"green\",\n  \"*lek*\" = \"orange\",\n  \"*ijssel*\" = \"blue\",\n  \"*zaan*\" = \"black\",\n  \"*waal*\" = \"magenta\"\n))"
  },
  {
    "objectID": "post/2021-02-06-riddler-another-hunt-for-mysterious-numbers/index.html",
    "href": "post/2021-02-06-riddler-another-hunt-for-mysterious-numbers/index.html",
    "title": "Riddler: Another Hunt For Mysterious Numbers",
    "section": "",
    "text": "This week’s FiveThirtyEight Riddler Express is similar to the Riddler Classic of two weeks ago:\n\nBy all accounts, Riddler Nation had a lot of fun hunting for the mysterious numbers a few weeks back. So here’s what we’re going to do: For the next four weeks, the Riddler Express will feature a similar puzzle that combines multiplication and logic. We’ll be calling these CrossProducts. For your first weekly CrossProduct, there are five three-digit numbers — each belongs in a row of the table below, with one digit per cell. The products of the three digits of each number are shown in the rightmost column. Meanwhile, the products of the digits in the hundreds, tens and ones places, respectively, are shown in the bottom row.\n\n\nTwo weeks ago, rather than searching for the solution manually, I solved this problem with linear programming in Julia. The advantage of that approach is that it is easy to apply to this new problem again — it should be a matter of changing the input numbers. Let’s see how it goes! Please refer to the aforementioned post for a more thorough explanation of the code.\nusing JuMP, GLPK\nusing Primes: factor, primes\nusing DataStructures: DefaultDict\n\nrow_prods = [135, 45, 64, 280, 70];\ncol_prods = [3_000, 3_969, 640];\nprms = primes(1, 9);\n\nn_prms = length(prms);\nn_rows = length(row_prods);\nn_cols = length(col_prods);\nNote that the only thing changed with respect to the previous post is that we initialized the problem with new row_prods and col_prods. The rest should be automatic.\nrow_factors = factor.(Dict, row_prods);\ncol_factors = factor.(Dict, col_prods);\n\nrow_factors = [[DefaultDict(0, row_factors[row])[prms[i]] for i in 1:4] for row in 1:n_rows]\ncol_factors = [[DefaultDict(0, col_factors[col])[prms[i]] for i in 1:4] for col in 1:n_cols]\n5-element Array{Array{Int64,1},1}:\n [0, 3, 1, 0]\n [0, 2, 1, 0]\n [6, 0, 0, 0]\n [3, 0, 1, 1]\n [1, 0, 1, 1]\n3-element Array{Array{Int64,1},1}:\n [3, 1, 3, 0]\n [0, 4, 0, 2]\n [7, 0, 1, 0]\nAgain, onto the actual problem definition.\nmodel = Model(with_optimizer(GLPK.Optimizer))\n\n@variable(model, 0 &lt;= x[1:n_rows, 1:n_cols, 1:n_prms], integer=true)\nMake sure row and column prime multiplicities add up to the correct numbers.\nfor i in 1:n_rows, p in 1:n_prms\n    @constraint(model, sum(x[i, :, p]) == row_factors[i][p])\nend\n\nfor j in 1:n_cols, p in 1:n_prms\n    @constraint(model, sum(x[:, j, p]) == col_factors[j][p])\nend\nFinally, we want every cell to be an integer between 1 and 9, which means 2^{p_2}\\cdot 3^{p_3}\\cdot 5^{p_5}\\cdot 7^{p_7} \\le 9, or p_2\\log(2) + p_3\\log(3) + p_5\\log(5)+p_7\\log(7) \\le \\log(9), a constraint which is linear in its variables.\nfor i in 1:n_rows, j in 1:n_cols\n    @constraint(model, sum(x[i, j, :] .* log.(prms)) &lt;= log(9))\nend\nAnd that is all! Let JuMP do its magic and come up with a feasible solution. As an example, again we print out the number of times a 2 (the first prime) appears in every cell.\nJuMP.optimize!(model)\n\nsolution = JuMP.value.(x);\n\nsolution[:, :, 1]\n5×3 Array{Float64,2}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 3.0  0.0  3.0\n 0.0  0.0  3.0\n 0.0  0.0  1.0\nThe final solution to the riddle is then the product of every prime to the power of its multiplicity:\n[[Int(prod(prms .^ solution[i, j, :])) for j in 1:n_cols] for i in 1:n_rows]\n5-element Array{Array{Int64,1},1}:\n [3, 9, 5]\n [5, 9, 1]\n [8, 1, 8]\n [5, 7, 8]\n [5, 7, 2]\nWell, that was easy! For the first puzzle, the challenge was to come up with a model. From then on, it is just a matter of changing the inputs. It does feel a bit like cheating, though … I don’t think I’ll continue with these CrossProduct puzzles in this way."
  },
  {
    "objectID": "post/2022-05-18-riddler-nonconformist-dice/index.html",
    "href": "post/2022-05-18-riddler-nonconformist-dice/index.html",
    "title": "Riddler: Nonconformist Dice",
    "section": "",
    "text": "Last week’s Riddler Classic is a question about rolling tetrahedral dice:\n\nYou have four fair tetrahedral dice whose four sides are numbered 1 through 4.\nYou play a game in which you roll them all and divide them into two groups: those whose values are unique, and those which are duplicates. For example, if you roll a 1, 2, 2 and 4, then the 1 and 4 will go into the “unique” group, while the 2s will go into the “duplicate” group.\nNext, you reroll all the dice in the duplicate pool and sort all the dice again. Continuing the previous example, that would mean you reroll the 2s. If the result happens to be 1 and 3, then the “unique” group will now consist of 3 and 4, while the “duplicate” group will have two 1s.\nYou continue rerolling the duplicate pool and sorting all the dice until all the dice are members of the same group. If all four dice are in the “unique” group, you win. If all four are in the “duplicate” group, you lose. What is your probability of winning the game?\n\nWe will answer this question using a Markov Chain in python. For that, we will use numpy (for matrix multiplication), collections.Counter (for easy counting) and itertools.product (for throwing dice).\n\nfrom collections import Counter\nfrom itertools import product\nimport numpy as np\n\nFirst, let’s calculate the probability of every possible throw with four tetrahedral dice. We will represent a dice throw as an ordered string.\n\n# all possible throws\nthrows = [\"\".join(str(i) for i in state) for state in product([1,2,3,4], repeat=4)]\n# aggregate identical throws and calculate probabilities\nall_states = [\"\".join(sorted(throw)) for throw in throws]\nfreqs = Counter(all_states).items()\nthrow = {k: v/(4**4) for k, v in freqs}\nprint(\"\\n\".join([f\"{k}: {v}\" for k, v in throw.items()][:12]) + \"\\n...\")\n\n1111: 0.00390625\n1112: 0.015625\n1113: 0.015625\n1114: 0.015625\n1122: 0.0234375\n1123: 0.046875\n1124: 0.046875\n1133: 0.0234375\n1134: 0.046875\n1144: 0.0234375\n1222: 0.015625\n1223: 0.046875\n...\n\n\nIn addition to these throw results, we have two states the chain can be in: \"loss\" and \"win\".\n\nstates = list(throw.keys()) + [\"loss\", \"win\"]\nstates\n\n['1111', '1112', '1113', '1114', '1122', '1123', '1124', '1133', '1134', '1144', '1222', '1223', '1224', '1233', '1234', '1244', '1333', '1334', '1344', '1444', '2222', '2223', '2224', '2233', '2234', '2244', '2333', '2334', '2344', '2444', '3333', '3334', '3344', '3444', '4444', 'loss', 'win']\n\n\nNext, we calculate the probability of going from one state to another. The following function will calculate all transition probabilities starting from state from_state. Remember we can roll every die in the “unique” group, but cannot change the die in the “duplicate” group. The states for \"loss\" and \"win\" are absorbing states.\n\ndef transition_probs(from_state):\n  # Return all transition probabilities from state `from_state`\n  \n  # initially fill all probabilities with zero\n  probs = {state: 0 for state in states}\n  \n  counts = Counter(from_state)\n  uniques = [k for k, v in counts.items() if v == 1]\n  \n  if len(uniques) == 0 or from_state == \"loss\":\n    probs[\"loss\"] = 1\n    return probs\n  elif len(uniques) == 4 or from_state == \"win\":\n    probs[\"win\"] = 1\n    return probs\n  \n  # number of dice to throw again\n  n_dice = 4 - len(uniques) \n  # calculate possible new states\n  draws = [\"\".join(str(i) for i in state) for state in product([1,2,3,4], repeat=n_dice)]\n  new_states = [\"\".join(sorted(draw + \"\".join(uniques))) for draw in draws]\n  # calculate new states' probabilities\n  freqs = Counter(new_states).items()\n  throw_probs = {k: v/(4**n_dice) for k, v in freqs}\n  \n  probs.update(throw_probs)\n  \n  return probs\n\nFor example, if we start with \"1112\", we throw the three \"1\" dice again, but keep the die with a \"2\". Therefore the probability mass is distributed over throws containing a \"2\".\n\ntransition_probs(\"1112\")\n\n{'1111': 0, '1112': 0.015625, '1113': 0, '1114': 0, '1122': 0.046875, '1123': 0.046875, '1124': 0.046875, '1133': 0, '1134': 0, '1144': 0, '1222': 0.046875, '1223': 0.09375, '1224': 0.09375, '1233': 0.046875, '1234': 0.09375, '1244': 0.046875, '1333': 0, '1334': 0, '1344': 0, '1444': 0, '2222': 0.015625, '2223': 0.046875, '2224': 0.046875, '2233': 0.046875, '2234': 0.09375, '2244': 0.046875, '2333': 0.015625, '2334': 0.046875, '2344': 0.046875, '2444': 0.015625, '3333': 0, '3334': 0, '3344': 0, '3444': 0, '4444': 0, 'loss': 0, 'win': 0}\n\n\nNext, we can generate a Markov matrix containing these transition probabilities for every starting state.\n\nmarkov_mat = [[v for k, v in transition_probs(state).items()] for state in states]\nmarkov_mat = np.asarray(markov_mat) # turn into numpy array for matrix multiplication\nmarkov_mat.shape\n\n(37, 37)\n\n\nTogether with an initial throw, we can then simulate a round of the game by multiplying the vector corresponding to the initial throw with the Markov matrix..\n\nfirst_throw = np.array([v for k, v in throw.items()] + [0, 0])\n\nres = first_throw @ markov_mat\ndict(zip(states, res))\n\n{'1111': 0.000732421875, '1112': 0.0087890625, '1113': 0.0087890625, '1114': 0.0087890625, '1122': 0.01611328125, '1123': 0.0380859375, '1124': 0.0380859375, '1133': 0.01611328125, '1134': 0.0380859375, '1144': 0.01611328125, '1222': 0.0087890625, '1223': 0.0380859375, '1224': 0.0380859375, '1233': 0.0380859375, '1234': 0.087890625, '1244': 0.0380859375, '1333': 0.0087890625, '1334': 0.0380859375, '1344': 0.0380859375, '1444': 0.0087890625, '2222': 0.000732421875, '2223': 0.0087890625, '2224': 0.0087890625, '2233': 0.01611328125, '2234': 0.0380859375, '2244': 0.01611328125, '2333': 0.0087890625, '2334': 0.0380859375, '2344': 0.0380859375, '2444': 0.0087890625, '3333': 0.000732421875, '3334': 0.0087890625, '3344': 0.01611328125, '3444': 0.0087890625, '4444': 0.000732421875, 'loss': 0.15625, 'win': 0.09375}\n\n\nWe can just as easily simulate multiple rounds of the game by taking the matrix to a certain power.\nAfter 20 rounds, it becomes apparent that the probability of winning seems to be 45%.\n\nres = first_throw @ np.linalg.matrix_power(markov_mat, 20)\ndict(zip(states, res))\n\n{'1111': 2.2522812066292564e-06, '1112': 3.60364993060681e-05, '1113': 3.60364993060681e-05, '1114': 3.60364993060681e-05, '1122': 6.756843619887768e-05, '1123': 0.00016216424687730643, '1124': 0.00016216424687730645, '1133': 6.756843619887768e-05, '1134': 0.00016216424687730648, '1144': 6.756843619887768e-05, '1222': 3.603649930606811e-05, '1223': 0.00016216424687730648, '1224': 0.00016216424687730648, '1233': 0.00016216424687730648, '1234': 0.0003783832427137152, '1244': 0.00016216424687730645, '1333': 3.603649930606811e-05, '1334': 0.00016216424687730648, '1344': 0.0001621642468773065, '1444': 3.603649930606811e-05, '2222': 2.2522812066292564e-06, '2223': 3.60364993060681e-05, '2224': 3.60364993060681e-05, '2233': 6.756843619887769e-05, '2234': 0.0001621642468773065, '2244': 6.756843619887769e-05, '2333': 3.6036499306068095e-05, '2334': 0.00016216424687730648, '2344': 0.00016216424687730648, '2444': 3.6036499306068095e-05, '3333': 2.2522812066292564e-06, '3334': 3.60364993060681e-05, '3344': 6.756843619887768e-05, '3444': 3.60364993060681e-05, '4444': 2.2522812066292564e-06, 'loss': 0.5483423210319212, 'win': 0.4484864670291449}\n\n\nSimulating the game for 1000 rounds confirms this.\n\nres = first_throw @ np.linalg.matrix_power(markov_mat, 1000)\ndict(zip(states, res))\n\n{'1111': 8.17825667702296e-129, '1112': 1.3085210683236735e-127, '1113': 1.3085210683236735e-127, '1114': 1.3085210683236735e-127, '1122': 2.4534770031068862e-127, '1123': 5.888344807456528e-127, '1124': 5.888344807456528e-127, '1133': 2.4534770031068862e-127, '1134': 5.888344807456528e-127, '1144': 2.4534770031068862e-127, '1222': 1.3085210683236735e-127, '1223': 5.888344807456528e-127, '1224': 5.888344807456528e-127, '1233': 5.888344807456528e-127, '1234': 1.3739471217398578e-126, '1244': 5.888344807456528e-127, '1333': 1.3085210683236735e-127, '1334': 5.888344807456528e-127, '1344': 5.888344807456528e-127, '1444': 1.3085210683236735e-127, '2222': 8.17825667702296e-129, '2223': 1.3085210683236735e-127, '2224': 1.3085210683236735e-127, '2233': 2.4534770031068862e-127, '2234': 5.888344807456528e-127, '2244': 2.4534770031068862e-127, '2333': 1.3085210683236735e-127, '2334': 5.888344807456528e-127, '2344': 5.888344807456528e-127, '2444': 1.3085210683236735e-127, '3333': 8.17825667702296e-129, '3334': 1.3085210683236735e-127, '3344': 2.4534770031068862e-127, '3444': 1.3085210683236735e-127, '4444': 8.17825667702296e-129, 'loss': 0.55, 'win': 0.44999999999999984}\n\n\nWhat’s interesting is that you have a higher probability of winning (48.3%) if you start with a pair.\n\ndict(zip(states, np.linalg.matrix_power(markov_mat, 1000)[:,-1]))\n\n{'1111': 0.0, '1112': 0.44999999999999996, '1113': 0.44999999999999996, '1114': 0.44999999999999996, '1122': 0.0, '1123': 0.48333333333333334, '1124': 0.48333333333333334, '1133': 0.0, '1134': 0.48333333333333334, '1144': 0.0, '1222': 0.44999999999999996, '1223': 0.48333333333333334, '1224': 0.48333333333333334, '1233': 0.48333333333333334, '1234': 1.0, '1244': 0.48333333333333334, '1333': 0.44999999999999996, '1334': 0.48333333333333334, '1344': 0.48333333333333334, '1444': 0.44999999999999996, '2222': 0.0, '2223': 0.44999999999999996, '2224': 0.44999999999999996, '2233': 0.0, '2234': 0.48333333333333334, '2244': 0.0, '2333': 0.44999999999999996, '2334': 0.48333333333333334, '2344': 0.48333333333333334, '2444': 0.44999999999999996, '3333': 0.0, '3334': 0.44999999999999996, '3344': 0.0, '3444': 0.44999999999999996, '4444': 0.0, 'loss': 0.0, 'win': 1.0}\n\n\nAnd that is how easy and elegant it can be to answer such a question using a Markov chain. Until next time!"
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html",
    "title": "Closest Eredivisie football club",
    "section": "",
    "text": "{r setup, warning=FALSE, include=FALSE, message=FALSE} knitr::opts_chunk$set(     message = FALSE,     warning = FALSE ) library(kableExtra) options(kableExtra.html.bsTable = TRUE)\nA couple of months back, I saw a map of England that showed the closest Premier League football club at any location. I can’t find the exact same map anymore, but this map on Reddit is similar.\nI figured it would be a nice exercise in spatial visualization in R to try and create a similar map for the Netherlands. The Dutch highest football league is called the Eredivisie and consists of 18 clubs. In this post, I will create a map conveying the closest Eredivisie club given a location. I will use the tidyverse for data wrangling and sf for spatial data transformations.\nFirst, we need the club’s locations. For this, we will use the addresses of the stadiums."
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html#python",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html#python",
    "title": "EleksDraw pen plotter with R",
    "section": "Python",
    "text": "Python\nMy initial python code was very hacky and unstable, so I decided to take a more structured approach, now that I knew I would be able to get it working. I found a nicely structured project by Michael Fogleman that does the same for the Makeblock XY Plotter. It defines an object that holds the connection to the plotter, and defines methods that send commands to it, like move(), pen_up() and pen_down(). It can render a drawing and store it as an image before sending it to the plotter, which makes it easy to assess an image virtually before physically drawing it. I forked the project, edited the device instructions and restructured it a bit, which resulted in EleksDrawPy.\nThis new code is a lot easier to use and more stable than my previous attempt. It also allows for usage from R via the reticulate R interface to Python."
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html#r",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html#r",
    "title": "EleksDraw pen plotter with R",
    "section": "R",
    "text": "R\nMy pen plotter was collecting dust when I stumbled upon fawkes, “an R interface to the AxiDraw plotter” by Thomas Lin Pedersen, who is part of RStudio’s tidyverse team. It uses reticulate to be able to use the AxiDraw python interface from R, and defines a device which one can write ggplot2 plots to, similar to R’s built-in png() or pdf() device. How cool would it be to be able to plot ggplot2 graphs on my plotter?\nIt turned out to be very straightforward to port his code to use my python code. The resulting code can be found on GitHub. I kept the original code intact, but added the eleks_dev() and the eleks_manual() functions, which are the counterparts of the axi_dev() and axi_manual() functions already present. The former can be used as a device like png(), the latter for interactive mode (i.e. sending individual commands to the device). I did not create a version of the axi_svg() function (that uses the AxiDraw svg plotting capabilities), since my EleksDraw python code does not have this functionality (yet).\nThe fawkes::ghost_dev() device that is also present in the package, makes it easy to get a feeling of what the resulting plot will look like. It shows not only the lines that will be drawn, but also the paths the device takes while the pen is in the air.\nNow, we can plot ggplot2 graphs from R right onto a piece of paper! Below are some examples. All credits go to Thomas Lin Pedersen and Michael Fogleman, whose code I merely adjusted for my specific goal."
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html#results",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html#results",
    "title": "EleksDraw pen plotter with R",
    "section": "Results",
    "text": "Results\nThe python module can be installed with pip install -r requirements.txt (after cloning the project), and the R package with remotes::install_github('basjacobs93/fawkes'). The packages we’ll use besides the aforementioned fawkes and ggplot2 are dplyr (for data manipulation) and sf (for using spatial data).\n\nlibrary(dplyr)\nlibrary(fawkes)\nlibrary(ggplot2)\nlibrary(sf)\n\nGiven a ggplot2 plot p, the plot can be previewed with the following,\n\ngd &lt;- ghost_dev('A6', portrait = FALSE, margins = 5, ignore_color = TRUE)\np\ninvisible(dev.off())\ngd$preview(plot_air = TRUE)\n\nand plotted with the EleksDraw using the below.\n\ngd &lt;- eleks_dev('A6', portrait = FALSE, margins = 5, ignore_color = TRUE)\np\ninvisible(dev.off())\n\nFor each of the below examples, we show the ggplot2 graph as a png, the output of the fawkes::ghost_dev() preview, and finally a picture of the plot on paper.\n\nFacets with mtcars\nThe first plot is copied straight from the fawkes examples. It plots the famous mtcars dataset using facets and demonstrates the capabilities of the fawkes library.\n\np &lt;- ggplot(mtcars) +\n  geom_point(aes(disp, mpg)) +\n  facet_wrap(~ gear) +\n  theme_bw(base_size = 6) +\n  theme(\n    plot.background = element_blank(),\n    panel.background = element_blank()\n  )\n\n\n\n\nmtcars, png\n\n\n\n\n\nmtcars, preview\n\n\n\n\n\nmtcars, pen on paper\n\n\n\n\nHarmonograph\nThe below code generates a random harmonograph, one instance of which I plotted.\n\nf1=jitter(sample(c(2,3),1));f2=jitter(sample(c(2,3),1));f3=jitter(sample(c(2,3),1));f4=jitter(sample(c(2,3),1))\nd1=runif(1,0,1e-02);d2=runif(1,0,1e-02);d3=runif(1,0,1e-02);d4=runif(1,0,1e-02)\np1=runif(1,0,pi);p2=runif(1,0,pi);p3=runif(1,0,pi);p4=runif(1,0,pi)\nharmonograph &lt;- data.frame(t = seq(0, 80*pi, 80*pi/10000)) %&gt;%\n  transmute(\n    x = exp(-d1*t)*sin(t*f1+p1) + exp(-d2*t)*sin(t*f2+p2),\n    y = exp(-d3*t)*sin(t*f3+p3) + exp(-d4*t)*sin(t*f4+p4)\n  )\n\np &lt;- harmonograph %&gt;%\n  ggplot(aes(x, y)) +\n  geom_path() +\n  theme_bw(base_size = 6) +\n  theme_void()\n\n\n\n\nHarmonograph, png\n\n\n\n\n\nHarmonograph, preview\n\n\n\n\n\nHarmonograph, pen on paper\n\n\n\n\nGeorg Nees - Schotter\nThe below generates falling squares like Schotter, by generative art pioneer Georg Nees. It can be seen that there was some slippage at the belt which resulted in incomplete squares. After this plot I tightened the belt, which made the subsequent plots a lot more accurate.\n\nn_cols &lt;- 12\nn_rows &lt;- 22\n\np &lt;- expand_grid(col = 0:(n_cols-1),\n            row = 0:(n_rows-1),\n            tibble(x = c(-0.5, 0.5,  0.5, -0.5),\n                   y = c( 0.5, 0.5, -0.5, -0.5))) %&gt;%\n  group_by(col, row) %&gt;%\n  mutate(angle = rnorm(1, sd = row/60),\n         xn = x*cos(angle) - y*sin(angle),\n         y = x*sin(angle) + y*cos(angle),\n         x = xn) %&gt;%\n  select(-xn) %&gt;%\n  mutate(x = x + 1 + col,\n         y = y + 1 - row,\n         x = x + rnorm(1)*row/80, # jitter\n         y = y + rnorm(1)*row/60) %&gt;%\n  mutate(xend = lag(x, default = last(x)),\n         yend = lag(y, default = last(y))) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_segment() +\n  coord_fixed() +\n  theme_void()\n\n\n\n\nSchotter, png\n\n\n\n\n\nSchotter, preview\n\n\n\n\n\nSchotter, pen on paper"
  },
  {
    "objectID": "post/2018-06-28-pen-plotter/index.html",
    "href": "post/2018-06-28-pen-plotter/index.html",
    "title": "Pen plotter",
    "section": "",
    "text": "Inspired by this blog post, I got interested in pen plotters and computer generated art. I liked the minimalist pieces of art a plotter could create, resembling a human hand doodling on a piece of paper. A lot of examples of this art can be found on Twitter at #plottertwitter. By far the most used plotter on Twitter is the AxiDraw, but since I didn’t want to spend over 400 euros and I liked the prospect of assembling something myself, I decided to buy the EleksDraw which was around 100 euros and came completely unassembled. In this blog post, I am summarising the steps I took to go from a box with nuts and bolts to some nice drawings on paper, hopefully inspiring you to try this out as well. Most of the code used can be found on my GitHub. Note that I wrote this code for myself, so please only use it as a reference and don’t use it without understanding what is going on."
  },
  {
    "objectID": "post/2018-06-28-pen-plotter/index.html#plotter-meets-python",
    "href": "post/2018-06-28-pen-plotter/index.html#plotter-meets-python",
    "title": "Pen plotter",
    "section": "Plotter meets Python",
    "text": "Plotter meets Python\nUsing the Python package pySerial, I was able to send commands to the plotter through just a couple of lines of code (see here):\n# Open serial port\ns = serial.Serial('/dev/wchusbserial1420', 115200)\n\n# Wake up\ns.write(\"\\r\\n\\r\\n\")\ntime.sleep(2)       # wait for grbl to initialize\ns.flushInput()      # flush startup text in serial input\n\ns.write(\"G21\\n\")   # units = mm\ns.readline()\n\ns.write(\"F5000\\n\") # speed = 5000 mm/min\ns.readline()\n\ns.write(\"S0 M5\\n\") # pen up\ns.readline()\n\ns.write(\"G01 X-10 Y-10\\n\") # go to (10, 10)\ns.readline()\n\ns.write(\"S1000 M3\\n\") # pen down\ns.readline()\n\ns.write(\"G01 X-20 Y-10\\n\") # go to (20, 10)\ns.readline()\n\ns.write(\"S0 M5\\n\") # pen up\ns.readline()\nThis piece of code wakes up the plotter, raises the pen, moves to (10, 10) (note I wrote -10 for reasons described above), lowers the pen, moves to (20, 10) and finally raises the pen again. Effectively, this draws a straight line from (10mm, 10mm) to (20mm, 10mm). Since I figured I would be needing this code more, I wrapped it in a function. I decided to go object-oriented and created classes Point and Line, respectively. For now, these classes only contained draw methods, but I knew more was going to be needed."
  },
  {
    "objectID": "post/2018-06-28-pen-plotter/index.html#lines-and-points",
    "href": "post/2018-06-28-pen-plotter/index.html#lines-and-points",
    "title": "Pen plotter",
    "section": "Lines and points",
    "text": "Lines and points\nWhat seemed most intuitive to me was converting the SVG &lt;path&gt; element to G-code instructions. Such an element can contain various types of lines and curves that together create a connected (closed or open) path. A path like this can be constructed by tracing an image in a tool like Inkscape or Sketch. My final goal was to be able to plot Picasso’s drawing ‘Dog’, which could be traced with a single path.\n\n\n\nDog (Picasso), pen on paper\n\n\nAn example of the SVG path syntax can be found on w3schools. It turned out that parsing such a path was straightforward with regular expressions. There were three things that made things slightly more complicated.\n\nDifferent instructions can have different numbers of parameters. For example, the M command (move) takes two numbers (an x and y coordinate), the Z (close path) command takes zero parameters, and some curve commands can take up to 7 parameters.\nA command letter does not need to be repeated. For example, L 10 20 10 30 draws a line from the current position to (10, 20) and then another line to (10, 30) and is the same as L 10 20 L 10 30.\nAll commands can also be expressed with lower case letters, referring to relative instead of absolute positions. This requires a post processing step in which we keep track of the current position and add it if a relative position was supplied.\n\nAll of these issues were relatively easy to overcome, and after implementing this, I was able to parse an SVG file into a table which I could then convert to G-code. The code for this parser can be found on my GitHub."
  },
  {
    "objectID": "post/2018-06-28-pen-plotter/index.html#bézier-curves",
    "href": "post/2018-06-28-pen-plotter/index.html#bézier-curves",
    "title": "Pen plotter",
    "section": "Bézier curves",
    "text": "Bézier curves\nStill, I had only created code for drawing points and lines. To plot arcs, I could of course interpolate these using points, but the results were what I expected them to be: not very pretty. G-code supports circle arcs (an arc obtained by tracing part of a circle), so a better way would be to try to approximate the curves using circle arcs. When tracing an image with a vector graphics tool like Sketch or Inkscape, the result will be a path consisting of Bézier curves. A Bézier curve is a parametric curve that can be described mathematically. I won’t go into more detail on Bézier curves; that is worth a blog post in itself. One thing that’s important to know is that there are quadratic and cubic Bézier curves. The quadratic curves are a special case of the cubic ones, so if I’m able to draw cubic Bézier curves I’m able to draw both.\n\n\n\nA cubic Bézier curve is defined by 4 points (source: Wikipedia)\n\n\nIt is in general not possible to convert a Bézier curve to a combination of circle arcs, but there are ways that are ‘good enough’. In particular, I implemented this approach in Python, which approximates a cubic Bézier curve with two circle arcs. I updated the classes for points and lines to include addition of points, multiplication of points with scalars, intersections of lines and so on. After these adjustments, it was relatively straightforward to write a CubicBezier class of which an instantiation could convert itself to two objects of class CircleArc. For debugging purposes, I gave every such class a method that could draw the corresponding shape on a PyGame canvas. That way, I had a way of testing if everything worked without having to go through stacks of paper. In the end, this also allowed me to check if the thing I was going to plot looked the way it should, before actually sending it to the EleksDraw. For example, find below the code for Line.\nimport plotter as plt\n\nclass Line():\n    # a line is defined by 2 points\n    def __init__(self, point1, point2):\n        self.P1 = point1\n        self.P2 = point2\n\n    def intersect(self, line2):\n        # calculates the intersection point of the lines\n        # solution is based on simple algebra\n        a = self.P1.x - self.P2.x\n        b = self.P1.y - self.P2.y\n\n        u = (a*(line2.P2.y-self.P2.y) - b*(line2.P2.x-self.P2.x)) /\\\n            (a*(line2.P2.y-line2.P1.y) - b*(line2.P2.x-line2.P1.x))\n\n        return (line2.P1.times(u)).plus(line2.P2.times(1-u))\n\n    def perpendicular_at(self, P):\n        U = Point(self.P2.y-self.P1.y, self.P1.x - self.P2.x)\n        return Line(P, P.plus(U))\n\n    def draw(self, canvas, color):\n        pygame.draw.line(canvas, color,\n                         (self.P1.x, canvas.get_height()-self.P1.y),\n                         (self.P2.x, canvas.get_height()-self.P2.y))\n\n    def plot_instructions(self):\n        # assumes we are at P1\n        return [plt.move(self.P2.x, self.P2.y)]\nAfter all this work, I was finally able to plot Picasso’s dog.\n\n\n\nDog (Picasso/EleksDraw), pen on paper"
  },
  {
    "objectID": "post/2018-10-28-showing-images-on-hover-in-plotly-with-r/index.html",
    "href": "post/2018-10-28-showing-images-on-hover-in-plotly-with-r/index.html",
    "title": "Showing images on hover in Plotly with R",
    "section": "",
    "text": "For a project I was working on recently, I wanted to turn a ggplot scatterplot into an interactive visualisation: when hovering over a point, a corresponding image needed to be shown. I did not want to use Shiny, since I required the visualisation to be portable. This is possible by manually tinkering with html, but using the plotly and htmlwidgets packages, I was able to achieve what I wanted without the need to leave the comfy RStudio environment, and without needing to host the plot on the plot.ly website.\nThe plotly library provides the useful ggplotly function to make static plots interactive with just one line of code. If we apply it to a ggplot of the famous iris dataset, it looks like this.\n\nlibrary(tidyverse)\nlibrary(plotly)\n\ng &lt;- ggplot(iris, aes(x = Sepal.Length,\n                      y = Petal.Length,\n                      color = Species)) + geom_point()\np &lt;- ggplotly(g) %&gt;% partial_bundle()\n\np\n\n\n\n\n\nAmong other things, we can now hover over a point on the graph and in the tooltip receive information about the corresponding data point. By default, the information displayed is exactly the information we define in the aes mapping. If we want other information, we can add it in the text aesthetic, which plotly can read. If we provide ggplotly with the tooltip = \"text\" option, this aesthetic is the only thing that is shown.\n\ng &lt;- ggplot(iris, aes(x = Sepal.Length,\n                      y = Petal.Length,\n                      color = Species,\n                      text = Species)) + geom_point()\np &lt;- ggplotly(g, tooltip = \"text\") %&gt;% partial_bundle() \n\np\n\n\n\n\n\nThis already looks nice and clean. As can be seen in the plotly documentation, a custom JavaScript function can be called when hovering over a point, and the tooltip text can be retrieved in this function. However, other than in the documentation, we do not need to change any html code or write long JavaScript code; using the htmltools::onRender function we can inject a custom JavaScript function into the generated plot.\nIn this example, I chose to store the images locally, but one can also use base64 objects like in the documentation to make it even more portable.\nWe define a function that takes a plotly element and calls another function when hovering over this element. The point’s tooltip can be retrieved with d.points[0].data.text. Since we made this nice and clean, this is the corresponding plant’s species as a string. Locally, I have stored the images in the folder corresponding to this blogpost, with filenames setosa.jpg, virginica.jpg and versicolor.jpg. The path to the correct image is constructed and assigned to the image_location variable.\nNext, we define an object which points to the correct image and defines the position and the size we want the image to take.\nFinally, by calling Plotly.relayout the new layout is applied in which we attach this image object in the layout’s images attribute.\n\np %&gt;% htmlwidgets::onRender(\"\n    function(el, x) {\n      // when hovering over an element, do something\n      el.on('plotly_hover', function(d) {\n\n        // extract tooltip text\n        txt = d.points[0].data.text;\n        // image is stored locally\n        image_location = '../2018-10-28-showing-images-on-hover-in-plotly-with-r/' + txt + '.jpg';\n\n        // define image to be shown\n        var img = {\n          // location of image\n          source: image_location,\n          // top-left corner\n          x: 0,\n          y: 1,\n          sizex: 0.2,\n          sizey: 0.2,\n          xref: 'paper',\n          yref: 'paper'\n        };\n\n        // show image and annotation \n        Plotly.relayout(el.id, {\n            images: [img] \n        });\n      })\n    }\n    \")\n\n\n\n\n\nTada! Hovering over a point now shows an image of the corresponding species in the top-left corner. Instead of an image, text can also be shown by adding a text attribute to the var img definition and adding annotations: [img] to the Plotly.relayout function.\nThis visualisation can now be exported to html with htmltools::saveWidget() and shared with anyone, the recipient does not need not have R installed. Do make sure to also share the folder with the images though, since these are not embedded."
  },
  {
    "objectID": "post/2020-10-02-riddler-can-you-eat-all-the-chocolates/index.html",
    "href": "post/2020-10-02-riddler-can-you-eat-all-the-chocolates/index.html",
    "title": "Riddler: Can You Eat All The Chocolates?",
    "section": "",
    "text": "Today’s 538 Riddler Classic is about eating chocolates:\nOur first approach will be a simple simulation to gauge what the answer should approximately be, before going to an analytical approach.\nAdded later: it turns out I misread the question. I assumed that after taking a chocolate that I could not eat, I would put it back and only eat the next one if it is of the same type. That is, however, not what’s asked: you always eat the following chocolate after you put one back, regardless of its type. I will keep my answer to the (wrong) question below for those who are interested."
  },
  {
    "objectID": "post/2020-10-02-riddler-can-you-eat-all-the-chocolates/index.html#simulation-in-r",
    "href": "post/2020-10-02-riddler-can-you-eat-all-the-chocolates/index.html#simulation-in-r",
    "title": "Riddler: Can You Eat All The Chocolates?",
    "section": "Simulation in R",
    "text": "Simulation in R\nThe following R function simulates eating one bag of chocolates and outputs what the last chocolate was.\n\nchocolates &lt;- c(rep(\"m\", 2), rep(\"d\", 8))\n\neat_chocolates &lt;- function() {\n  # shuffle the chocolates around\n  chocolates_left &lt;- sample(chocolates)\n\n  # eat the first chocolate\n  last_taken &lt;- chocolates_left[1]\n  chocolates_left &lt;- chocolates_left[-1]\n\n  # eat the rest of the chocolates\n  while (length(chocolates_left) &gt; 0) {\n    # take the first chocolate out of the bag\n    to_eat &lt;- chocolates_left[1]\n    chocolates_left &lt;- chocolates_left[-1]\n\n    # put back and shuffle if differs from previous piece\n    if (to_eat != last_taken) {\n      chocolates_left &lt;- sample(c(to_eat, chocolates_left))\n    }\n\n    last_taken &lt;- to_eat\n  }\n\n  last_taken\n}\n\nWe can now easily run, for example, 100,000 simulations and calculate the percentage in which the last chocolate was milk:\n\nsimulations &lt;- replicate(1e5, eat_chocolates())\nmean(simulations == \"m\")\n\n[1] 0.45907\n\n\nIn a couple of seconds, this gives an estimate of a 46% probability of ending with a piece of milk chocolate. This is closer to 50% than I initially expected. Intuitively though, it does make sense, given the nature of the problem that the majority chocolate type will be eaten relatively quickly, since it is easier to get a ‘streak’ of those.\nRunning this (admittedly naive) simulation for more than, say, 1 million replications quickly becomes intractable. For a more accurate result that does run quickly, we can resort to recursion."
  },
  {
    "objectID": "post/2020-10-02-riddler-can-you-eat-all-the-chocolates/index.html#recursion",
    "href": "post/2020-10-02-riddler-can-you-eat-all-the-chocolates/index.html#recursion",
    "title": "Riddler: Can You Eat All The Chocolates?",
    "section": "Recursion",
    "text": "Recursion\nThe problem ca be written in a recursive form: given a number of milk and dark chocolates, we can create a set of possible next states and attach probabilities to those states. We can repeat this until we arrive at a bag of only one type of chocolate.\nIn order to do that, let’s first introduce some notation. Let P(m, d, x) be the probability of ending with a milk chocolate given that we currently have a bag of m milk and d dark chocolates and that the last chocolate we ate was of type x\\in\\{\\text{milk}, \\text{dark}\\}. Denote by x^c the chocolate type that is not x. We then know the trivial cases P(m, 0, x) = 1 and P(0,d,x)=0; you can’t “fail” if you only have milk left, and you can’t “succeed” if you only have dark chocolates left.\nMoreover, the probability of ending with a milk chocolate given a certain distribution of chocolates and a previous pick equals the probability of ending with a dark chocolate given that we have switched all dark and milk chocolates around. In mathematical terms, P(m,d|x) = 1-P(d,m|x^c).\nNow, given a state (m,d, \\text{milk}), you will pick a milk chocolate with probability \\frac{m}{m+d}, resulting in a state of (m-1, d, \\text{milk}), or you will pick dark with probability \\frac{d}{m+d}, resulting in (m, d, \\text{dark}). In other words, given m,d&gt;0,\nP(m, d, \\text{milk}) = \\frac{m}{m+d}\\cdot P(m-1,d,\\text{milk})\\,+\\,\\frac{d}{m+d}\\cdot P(m,d,\\text{dark}).\n\nWe can now immediately see a problem with programming this recursion as it is: the second term did not decrease the total number of chocolates. Were we to do another step, we could again arrive at the situation we started with, thus resulting in an endless loop. In other words, theoretically we could keep alternately picking \\text{milk} and \\text{dark} chocolates and never finish the whole bag.\nWe need to go deeper.\nLet’s do another recursion step by expanding the right-most term in the above equation:\n\nP(m,d,\\text{dark}) = \\frac{m}{m+d}\\cdot P(m,d,\\text{milk})\\,+\\,\\frac{d}{m+d}\\cdot P(m,d-1,\\text{dark}).\n\nFilling this into the first equation,\n\nP(m, d, \\text{milk}) = \\frac{m}{m+d}\\cdot P(m-1,d,\\text{milk})\\,+\\,\\frac{d}{m+d}\\cdot \\left(\\frac{m}{m+d}\\cdot P(m,d,\\text{milk})\\,+\\,\\frac{d}{m+d}\\cdot P(m,d-1,\\text{dark})\\right),\n\nwhich we can rearrange into\n\nP(m, d, \\text{milk}) - \\frac{md}{(m+d)^2}\\cdot P(m,d,\\text{milk}) = \\frac{m}{m+d}\\cdot P(m-1,d,\\text{milk})\\,+\\,\\frac{d^2}{(m+d)^2}\\cdot P(m,d-1,\\text{dark}),\n\nor\n\n\\frac{(m+d)^2-md}{(m+d)^2}\\cdot P(m,d,\\text{milk}) = \\frac{m}{m+d}\\cdot P(m-1,d,\\text{milk})\\,+\\,\\frac{d^2}{(m+d)^2}\\cdot P(m,d-1,\\text{dark}),\n\nwhich gives\n\nP(m,d,\\text{milk}) = \\frac{m^2+md}{m^2+d^2+md}\\cdot P(m-1,d,\\text{milk})\\,+\\,\\frac{d^2}{m^2+d^2+md}\\cdot P(m,d-1,\\text{dark}).\n\nNow, by magic, this recursion does strictly decrease the total number of chocolates in the bag, thus making a program feasible. Together with the relationship P(m,d,\\text{dark})=1-P(d,m,\\text{milk}), we can calculate our quantity of interest, which is the probability of ending with a milk chocolate given a starting bag and no previous chocolates eaten:\n\nP(m,d)=\\frac{m}{m+d}\\cdot P(m-1,d,\\text{milk})\\,+\\,\\frac{d}{m+d}\\cdot P(m,d-1,\\text{dark}).\n\nIn R code, this recursion can be programmed as follows.\n\nP &lt;- function(m, d, previous = \"\") {\n  if (m == 0) {\n    return(0)\n  } else if (d == 0) {\n    return(1)\n  }\n\n  if (previous == \"\") {\n    m/(m+d)*P(m-1, d, \"milk\") + d/(m+d)*P(m, d-1, \"dark\")\n  } else if (previous == \"milk\") {\n    (m^2+m*d)/(m^2+d^2+m*d)*P(m-1, d, \"milk\") + (d^2)/(m^2+d^2+m*d)*P(m, d-1, \"dark\")\n  } else if (previous == \"dark\") {\n    1 - P(d, m, \"milk\")\n  }\n}\n\nThis allows us to calculate some simple examples,\n\nP(2,0)\n\n[1] 1\n\nP(0,2)\n\n[1] 0\n\nP(1,1)\n\n[1] 0.5\n\n\nbut also the actual quantity of interest\n\nP(2,8)\n\n[1] 0.4615372\n\n\nwithin a second. This agrees with the outcome of our simulation: the probability of ending with a milk chocolate is about 46.15%."
  },
  {
    "objectID": "post/2020-10-02-riddler-can-you-eat-all-the-chocolates/index.html#other-starting-points",
    "href": "post/2020-10-02-riddler-can-you-eat-all-the-chocolates/index.html#other-starting-points",
    "title": "Riddler: Can You Eat All The Chocolates?",
    "section": "Other starting points",
    "text": "Other starting points\nNow that we have code that runs quickly, we can explore the probabilities for other mixes of chocolates.\nSay we start with 2 milk chocolates, but vary the number of dark chocolates.\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\ntheme_set(theme_light())\n\nsimulations &lt;- tibble(m = 2, d = 1:100) %&gt;%\n  rowwise() %&gt;%\n  mutate(p = P(m, d))\n\nggplot(simulations, aes(d, p)) +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(title = \"Probability of ending with a milk chocolate\",\n       subtitle = \"Given 2 milk and a varying number of dark chocolates\",\n       x = \"Number of dark chocolates\",\n       y = \"Probability\")\n\n\n\n\n\n\n\n\nAt first, the probability of success drops quickly, but at some point the streaks of dark chocolates will be so common that adding more dark chocolates does not decrease this probability significantly.\nWe can do the same thing in two dimensions by also varying the number of milk chocolates. Now, memoization comes in handy, since otherwise we calculate the same probability many times. The function P_mem calculates P but checks whether a certain probability has already been calculated in the list of matrices mem.\n\nn_max &lt;- 20\nmem &lt;- list(milk = matrix(nrow = n_max, ncol = n_max),\n            dark = matrix(nrow = n_max, ncol = n_max))\n\nP_mem &lt;- function(m, d, previous = \"\") {\n  if (m == 0) {\n    return(0)\n  } else if (d == 0) {\n    return(1)\n  }\n\n  if (previous != \"\" && !is.na(mem[[previous]][m, d])) {\n    return(mem[[previous]][m, d])\n  }\n\n  if (previous == \"\") {\n    res &lt;- m/(m+d)*P_mem(m-1, d, \"milk\") + d/(m+d)*P_mem(m, d-1, \"dark\")\n  } else if (previous == \"milk\") {\n    res &lt;- (m^2+m*d)/(m^2+d^2+m*d)*P_mem(m-1, d, \"milk\") + (d^2)/(m^2+d^2+m*d)*P_mem(m, d-1, \"dark\")\n    mem[[previous]][m, d] &lt;&lt;- res\n  } else if (previous == \"dark\") {\n    res &lt;- 1 - P_mem(d, m, \"milk\")\n    mem[[previous]][m, d] &lt;&lt;- res\n  }\n\n  res\n}\n\nsimulations &lt;- crossing(m = 1:n_max, d = 1:n_max) %&gt;%\n  rowwise() %&gt;%\n  mutate(p = P_mem(m, d))\n\nggplot(simulations, aes(m, d, fill = p)) +\n  geom_raster() +\n  scale_fill_continuous(labels = scales::percent_format()) +\n  labs(title = \"Probability of ending with a milk chocolate\",\n       x = \"Number of milk chocolates\",\n       y = \"Number of dark chocolates\",\n       fill = \"Probability\")\n\n\n\n\n\n\n\n\nThis graph shows that the more chocolates there are in the bag, the smaller the differences between different mixes become."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bas Jacobs",
    "section": "",
    "text": "Dutch placename mapper\n\n\n\n\n\nApr 28, 2024\n\n\n\n\n\n\n\nPhysics-informed neural network for population dynamics\n\n\n\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\nRiddler: Nonconformist Dice\n\n\n\n\n\nMay 18, 2022\n\n\n\n\n\n\n\nRiddler: Another Hunt For Mysterious Numbers\n\n\n\n\n\nFeb 6, 2021\n\n\n\n\n\n\n\nRiddler: Can You Hunt For The Mysterious Numbers?\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n\nEleksDraw pen plotter with R\n\n\n\n\n\nJan 4, 2021\n\n\n\n\n\n\n\nImage Triangulation with Julia\n\n\n\n\n\nNov 18, 2020\n\n\n\n\n\n\n\nRiddler: Can You Eat All The Chocolates?\n\n\n\n\n\nOct 2, 2020\n\n\n\n\n\n\n\nTrees and Tents\n\n\n\n\n\nApr 21, 2019\n\n\n\n\n\n\n\nShowing images on hover in Plotly with R\n\n\n\n\n\nOct 28, 2018\n\n\n\n\n\n\n\nLearning image representation with Keras in R\n\n\n\n\n\nSep 20, 2018\n\n\n\n\n\n\n\nGenerating the logo for this site\n\n\n\n\n\nJul 15, 2018\n\n\n\n\n\n\n\nPen plotter\n\n\n\n\n\nJun 28, 2018\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/index.html",
    "href": "post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/index.html",
    "title": "Physics-informed neural network for population dynamics",
    "section": "",
    "text": "A Physics-Informed Neural Network (PINN) is a neural network that incorporates knowledge about physical laws alongside data. These physical (or chemical, or biological) laws can be incorporated into the network in the form of differential equations. Among other things, PINNs can be used to estimate parameters of a differential equation based on observational data.\nLast week, Dr. Riccardo Taormina gave a guest lecture in my organization in which he explained the workings and advantages of PINNs. In the accompanying workshop he used synthetic data to show that it is possible to estimate the parameters of differential equations of a harmonic oscillator and an advection–diffusion process.\nI once read an article by Bob Carpenter about predator-prey population dynamics of hares and lynxes in Canada. The interesting thing of this article is that it uses actual observations rather than synthetical data. The author uses data for the number of pelts of these hares and lynxes obtained by Hudson’s Bay Company in the years 1900-1920 to estimate the parameters of a Lotka-Volterra model. He uses the statistical modeling software Stan to generate these estimates based on Markov-Chain Monte-Carlo. This gives posterior estimates (distributions), which elegantly account for measurement and estimation uncertainty. In the Joseph M. Mahaffy course Mathematical Modeling, the author uses an optimization routine to estimate the parameters of the Lotka-Volterra equations using the same dataset.\nIn this post, I will show that PINNs can also be used to estimate those parameters. Let’s get started!"
  },
  {
    "objectID": "post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/index.html#packages",
    "href": "post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/index.html#packages",
    "title": "Physics-informed neural network for population dynamics",
    "section": "Packages",
    "text": "Packages\nWe will use the Python language with packages Pandas to load the data, Torch to train the neural network and M:w plotlib for plotting.\n%matplotlib inline\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/index.html#the-data",
    "href": "post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/index.html#the-data",
    "title": "Physics-informed neural network for population dynamics",
    "section": "The data",
    "text": "The data\nThe data can be downloaded from various places, all derived from http://www.math.tamu.edu/~phoward/m442/modbasics.pdf. I load it from here. The dataset consists of observations of lynxes and Hare pelts (in thousands) across the years 1900 to 1920.\nurl = \"https://raw.githubusercontent.com/cas-bioinf/statistical-simulations/master/hudson-bay-lynx-hare.csv\"\ndf = pd.read_csv(url, sep=\",\\\\s\", skiprows=2, index_col=0)\ndf\n\n\n\n\n\n\n\n\nLynx\n\n\nHare\n\n\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n1900\n\n\n4.0\n\n\n30.0\n\n\n\n\n1901\n\n\n6.1\n\n\n47.2\n\n\n\n\n1902\n\n\n9.8\n\n\n70.2\n\n\n\n\n1903\n\n\n35.2\n\n\n77.4\n\n\n\n\n1904\n\n\n59.4\n\n\n36.3\n\n\n\n\n1905\n\n\n41.7\n\n\n20.6\n\n\n\n\n1906\n\n\n19.0\n\n\n18.1\n\n\n\n\n1907\n\n\n13.0\n\n\n21.4\n\n\n\n\n1908\n\n\n8.3\n\n\n22.0\n\n\n\n\n1909\n\n\n9.1\n\n\n25.4\n\n\n\n\n1910\n\n\n7.4\n\n\n27.1\n\n\n\n\n1911\n\n\n8.0\n\n\n40.3\n\n\n\n\n1912\n\n\n12.3\n\n\n57.0\n\n\n\n\n1913\n\n\n19.5\n\n\n76.6\n\n\n\n\n1914\n\n\n45.7\n\n\n52.3\n\n\n\n\n1915\n\n\n51.1\n\n\n19.5\n\n\n\n\n1916\n\n\n29.7\n\n\n11.2\n\n\n\n\n1917\n\n\n15.8\n\n\n7.6\n\n\n\n\n1918\n\n\n9.7\n\n\n14.6\n\n\n\n\n1919\n\n\n10.1\n\n\n16.2\n\n\n\n\n1920\n\n\n8.6\n\n\n24.7\n\n\n\n\n\nThe data follow a cyclical pattern in which increases in the population of hares are followed by increases in the lynx populations in the subsequent years.\ndf[[\"Hare\", \"Lynx\"]].plot(figsize=(10,6), grid=True, xticks = df.index.astype(int)[::2], title=\"Hudson Bay Lynx-Hare Dataset\", ylabel=\"Number of pelts (thousands)\")"
  },
  {
    "objectID": "post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/index.html#the-lotka-volterra-model",
    "href": "post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/index.html#the-lotka-volterra-model",
    "title": "Physics-informed neural network for population dynamics",
    "section": "The Lotka-Volterra model",
    "text": "The Lotka-Volterra model\nThe Lotka-Volterra population model the change of predator and pray populations over time: - u(t)\\ge0 is the population size of the prey species at time t (the hare in our case), and - v(t)\\ge0 is the population size of the predator species (the lynx in our case).\nBy using such a model, some assumptions are made on the dynamics of these populations. We will not go into those here, but they can be found on the Wikipedia page. The population sizes over times of the two species are modelled in terms of four parameters, \\alpha,\\beta,\\gamma,\\delta\\ge0 as\n\n\\frac{\\mathrm{d}}{\\mathrm{d}t} u = \\alpha u - \\beta u v\n \n\\frac{\\mathrm{d}}{\\mathrm{d}t} v = \\delta uv - \\gamma v\n\nIt is these four parameters we will estimate in this post. They have the following interpretations: - \\alpha is the growth rate of the prey population, - \\beta is the rate of shrinkage relative to the product of the population sizes, - \\gamma is the shrinkage rate of the predator population, - \\delta is the growth rate of the predator population as a factor of the product of the population sizes.\nIn the Stan article, the author obtains posterior mean point estimates\n\n\\hat{\\alpha} = 0.55,\\quad\n\\hat{\\beta} = 0.028,\\quad\n\\hat{\\gamma} = 0.80,\\quad\n\\hat{\\delta} = 0.024,\n and in the Mathematical Modeling course, the author obtains \n\\alpha^* = 0.55,\\quad\n\\beta^* = 0.028,\\quad\n\\gamma^* = 0.84,\\quad\n\\delta^* = 0.026.\n\nTo begin, we first set up some plotting functionality for keeping track of the training progress.\ndef plot_result(i, t, y, yh, loss, xp=None, lossp=None, params=None):\n    plt.figure(figsize=(8, 4))\n    plt.xlim(0, 40)\n    plt.ylim(0, 85)\n\n    # Data\n    plt.plot(t, y, \"o-\", linewidth=3, alpha=0.2, label=[\"Exact hare\", \"Exact lynx\"])\n    plt.gca().set_prop_cycle(None)  # reset the colors\n    plt.plot(t, yh, linewidth=1, label=[\"Prediction hare\", \"Prediction lynx\"])\n\n    # Physics training ticks\n    if xp is not None:\n        plt.scatter(xp, 0*torch.ones_like(xp), s=3, color=\"tab:green\", alpha=1, marker=\"|\", label=\"Physics loss timesteps\")\n\n    # Legend\n    l = plt.legend(loc=(0.56, 0.1), frameon=False, fontsize=\"large\")\n    plt.setp(l.get_texts(), color=\"k\")\n\n    # Title\n    plt.text(23, 73, f\"Training step: {i}\", fontsize=\"xx-large\", color=\"k\")\n    rmse_text = f\"RMSE data: {int(torch.round(loss))}\"\n    if lossp is not None:\n        rmse_text += f\", physics: {int(torch.round(lossp))}\"\n    plt.text(23, 66, rmse_text, fontsize=\"medium\", color=\"k\")\n    if params is not None:\n        plt.text(23, 60, params, fontsize=\"medium\", color=\"k\")\n\n    plt.axis(\"off\")\n    plt.show()\nThe inputs for the model are the timesteps t=0..20 corresponding to the years 1900-1920 and the outputs are the numbers of hares and lynxes.\ndf[\"Timestep\"] = df.index - min(df.index)\nt = torch.tensor(df[\"Timestep\"].values, dtype=torch.float32).view(-1,1)\ny = torch.tensor(df[[\"Hare\", \"Lynx\"]].values, dtype=torch.float32)\nFirst, we define a simple dense feed-forward neural network using Torch.\nclass NN(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_layers):\n        super().__init__()\n        self.encode = nn.Sequential(nn.Linear(n_input, n_hidden), nn.Tanh())\n        self.hidden = nn.Sequential(*[nn.Sequential(nn.Linear(n_hidden, n_hidden), nn.Tanh()) for _ in range(n_layers-1)])\n        self.decode = nn.Linear(n_hidden, n_output)\n\n    def forward(self, x):\n        x = self.encode(x)\n        x = self.hidden(x)\n        x = self.decode(x)\n        return x\nWe can train neural network on the dataset (predicting the number of lynx and hares based on timestep t) and see that it is complex enough to be able to fit the dataset well. This takes a couple of seconds on my machine.\ntorch.manual_seed(123)\nmodel = NN(1, 2, 8, 2)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nfor i in range(50000):\n    optimizer.zero_grad()\n\n    yh = model(t)\n    loss = torch.mean((yh - y)**2)\n\n    loss.backward()\n    optimizer.step()\n    if i%10000 == 0:\n        yh = model(t).detach()\n\n        plot_result(i, t, y, yh, loss, None)\n\n\n\n\n\nNext, we use exactly the same architecture but add parameters alpha, beta, gamma and delta reflecting the parameters in the Lotka-Volterra equations. We initialize them randomly between 0 and 1, and add them to the list of model paramters so that they are updated in the backpropagation steps.\ntorch.manual_seed(123)\nmodel = NN(1, 2, 8, 2)\n\nalpha = torch.rand(1, requires_grad=True)\nbeta = torch.rand(1, requires_grad=True)\ngamma = torch.rand(1, requires_grad=True)\ndelta = torch.rand(1, requires_grad=True)\n\nextra_parameters = [alpha, beta, gamma, delta]\nparameters = list(model.parameters()) + extra_parameters\noptimizer = torch.optim.Adam(parameters, lr=1e-3)\nWe define t_physics; the range of timesteps on which we numerically evaluate the differential equations. In this case, we evaluate them every 0.1 year.\nt_physics = torch.arange(0, len(t), 0.1, dtype=torch.float32).view(-1,1).requires_grad_(True)\nTraining the PINN is identical to training the normal neural network, except that the loss function consists of a combination of the data loss and the physics loss. The data loss is identical to the loss of the previous neural network. For the physics loss, note that we can rewrite the Lotka-Volterra equations as \n\\frac{\\mathrm{d}}{\\mathrm{d}t} u - (\\alpha u - \\beta u v) = 0,\n \n\\frac{\\mathrm{d}}{\\mathrm{d}t} v - (\\delta uv - \\gamma v) = 0.\n We calculate the mean of the square of the left-hand side across the values of t in t_physics, thus punishing the model if the equations are far away from 0. We use torch.autograd to estimate the partial derivatives, which means that these partial derivatives are based on what the neural network has learned so far. We calculate a weighted sum of the physics and data loss, using a weight of lambda1 set to 0.01. The value of this weight can matter quite a lot and depends on the relative sizes of the errors. Intuitively, it captures how much you trust the data versus the physical laws.\nlambda1 = 1e-2\nfor i in range(50000):\n    optimizer.zero_grad()\n\n    # Data loss\n    yh = model(t)\n    data_loss = torch.mean((yh - y)**2)\n\n    # Physics loss\n    yhp = model(t_physics)  # output of the model at the t_physics timesteps\n    u, v = yhp[:, 0], yhp[:, 1]  # hare and lynx populations according to the neural network\n    dudt  = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0].flatten() # time derivative of hare\n    dvdt = torch.autograd.grad(v,  t_physics, torch.ones_like(v),  create_graph=True)[0].flatten() # time derivative of lynx\n    dudt_loss = torch.mean((dudt - (alpha*u - beta*u*v))**2)\n    dvdt_loss = torch.mean((dvdt - (delta*u*v - gamma*v))**2)\n    physics_loss = dudt_loss + dvdt_loss\n\n    loss = data_loss + lambda1*physics_loss\n    loss.backward()\n    optimizer.step()\n\n    if i%10000 == 0:\n        yh = model(t).detach()\n        tp = t_physics.detach()\n\n        a, b, c, d = [round(param.item(), 3) for param in extra_parameters]\n        params = rf\"$\\alpha={a}, \\beta={b}, \\gamma={c}, \\delta={d}$\"\n        plot_result(i, t, y, yh, loss, tp, physics_loss, params)\n\n\n\n\n\nWe see that the model is still able to fit well (albeit a little bit worse), and is able to estimate values for the parameters. Specifically, it estimates\n\n\\hat{\\alpha} = 0.57,\\quad\n\\hat{\\beta} = 0.027,\\quad\n\\hat{\\gamma} = 0.94,\\quad\n\\hat{\\delta} = 0.026,\n which are very close to the results of the other two approaches mentioned above. Only the estimate for \\gamma deviates significantly from the other estimates.\nHowever, this approach is very simple to implement and requires no knowledge about the differential equations or Bayesian modelling software.\nThanks for reading!"
  },
  {
    "objectID": "post/2021-01-04-eleksdraw-pen-plotter-with-r/index.html",
    "href": "post/2021-01-04-eleksdraw-pen-plotter-with-r/index.html",
    "title": "EleksDraw pen plotter with R",
    "section": "",
    "text": "The EleksDraw pen plotter is a relatively cheap pen plotter that works like the better-known AxiDraw plotter. Unlike the AxiDraw, it comes as a kit that needs to be assembled, and the only software available is a Windows program. Since I’m using macOS and I wanted more freedom, I decided to fiddle around with it in Python, which resulted in a blog post and accompanying code. In this post, I describe how I got to get it working with ggplot2 in R."
  },
  {
    "objectID": "post/2021-01-04-eleksdraw-pen-plotter-with-r/index.html#python",
    "href": "post/2021-01-04-eleksdraw-pen-plotter-with-r/index.html#python",
    "title": "EleksDraw pen plotter with R",
    "section": "Python",
    "text": "Python\nMy initial python code was very hacky and unstable, so I decided to take a more structured approach, now that I knew I would be able to get it working. I found a nicely structured project by Michael Fogleman that does the same for the Makeblock XY Plotter. It defines an object that holds the connection to the plotter, and defines methods that send commands to it, like move(), pen_up() and pen_down(). It can render a drawing and store it as an image before sending it to the plotter, which makes it easy to assess an image virtually before physically drawing it. I forked the project, edited the device instructions and restructured it a bit, which resulted in EleksDrawPy.\nThis new code is a lot easier to use and more stable than my previous attempt. It also allows for usage from R via the reticulate R interface to Python."
  },
  {
    "objectID": "post/2021-01-04-eleksdraw-pen-plotter-with-r/index.html#r",
    "href": "post/2021-01-04-eleksdraw-pen-plotter-with-r/index.html#r",
    "title": "EleksDraw pen plotter with R",
    "section": "R",
    "text": "R\nMy pen plotter was collecting dust when I stumbled upon fawkes, “an R interface to the AxiDraw plotter” by Thomas Lin Pedersen, who is part of RStudio’s tidyverse team. It uses reticulate to be able to use the AxiDraw python interface from R, and defines a device which one can write ggplot2 plots to, similar to R’s built-in png() or pdf() device. How cool would it be to be able to plot ggplot2 graphs on my plotter?\nIt turned out to be very straightforward to port his code to use my python code. The resulting code can be found on GitHub. I kept the original code intact, but added the eleks_dev() and the eleks_manual() functions, which are the counterparts of the axi_dev() and axi_manual() functions already present. The former can be used as a device like png(), the latter for interactive mode (i.e. sending individual commands to the device). I did not create a version of the axi_svg() function (that uses the AxiDraw svg plotting capabilities), since my EleksDraw python code does not have this functionality (yet).\nThe fawkes::ghost_dev() device that is also present in the package, makes it easy to get a feeling of what the resulting plot will look like. It shows not only the lines that will be drawn, but also the paths the device takes while the pen is in the air.\nNow, we can plot ggplot2 graphs from R right onto a piece of paper! Below are some examples. All credits go to Thomas Lin Pedersen and Michael Fogleman, whose code I merely adjusted for my specific goal."
  },
  {
    "objectID": "post/2021-01-04-eleksdraw-pen-plotter-with-r/index.html#results",
    "href": "post/2021-01-04-eleksdraw-pen-plotter-with-r/index.html#results",
    "title": "EleksDraw pen plotter with R",
    "section": "Results",
    "text": "Results\nThe python module can be installed with pip install -r requirements.txt (after cloning the project), and the R package with remotes::install_github('basjacobs93/fawkes'). The packages we’ll use besides the aforementioned fawkes and ggplot2 are dplyr (for data manipulation) and sf (for using spatial data).\n\nlibrary(dplyr)\nlibrary(fawkes)\nlibrary(ggplot2)\nlibrary(sf)\n\nGiven a ggplot2 plot p, the plot can be previewed with the following,\n\ngd &lt;- ghost_dev('A6', portrait = FALSE, margins = 5, ignore_color = TRUE)\np\ninvisible(dev.off())\ngd$preview(plot_air = TRUE)\n\nand plotted with the EleksDraw using the below.\n\ngd &lt;- eleks_dev('A6', portrait = FALSE, margins = 5, ignore_color = TRUE)\np\ninvisible(dev.off())\n\nFor each of the below examples, we show the ggplot2 graph as a png, the output of the fawkes::ghost_dev() preview, and finally a picture of the plot on paper.\n\nFacets with mtcars\nThe first plot is copied straight from the fawkes examples. It plots the famous mtcars dataset using facets and demonstrates the capabilities of the fawkes library.\n\np &lt;- ggplot(mtcars) +\n  geom_point(aes(disp, mpg)) +\n  facet_wrap(~ gear) +\n  theme_bw(base_size = 6) +\n  theme(\n    plot.background = element_blank(),\n    panel.background = element_blank()\n  )\n\n\n\n\nmtcars, png\n\n\n\n\n\nmtcars, preview\n\n\n\n\n\nmtcars, pen on paper\n\n\n\n\nHarmonograph\nThe below code generates a random harmonograph, one instance of which I plotted.\n\nf1=jitter(sample(c(2,3),1));f2=jitter(sample(c(2,3),1));f3=jitter(sample(c(2,3),1));f4=jitter(sample(c(2,3),1))\nd1=runif(1,0,1e-02);d2=runif(1,0,1e-02);d3=runif(1,0,1e-02);d4=runif(1,0,1e-02)\np1=runif(1,0,pi);p2=runif(1,0,pi);p3=runif(1,0,pi);p4=runif(1,0,pi)\nharmonograph &lt;- data.frame(t = seq(0, 80*pi, 80*pi/10000)) %&gt;%\n  transmute(\n    x = exp(-d1*t)*sin(t*f1+p1) + exp(-d2*t)*sin(t*f2+p2),\n    y = exp(-d3*t)*sin(t*f3+p3) + exp(-d4*t)*sin(t*f4+p4)\n  )\n\np &lt;- harmonograph %&gt;%\n  ggplot(aes(x, y)) +\n  geom_path() +\n  theme_bw(base_size = 6) +\n  theme_void()\n\n\n\n\nHarmonograph, png\n\n\n\n\n\nHarmonograph, preview\n\n\n\n\n\nHarmonograph, pen on paper\n\n\n\n\nGeorg Nees - Schotter\nThe below generates falling squares like Schotter, by generative art pioneer Georg Nees. It can be seen that there was some slippage at the belt which resulted in incomplete squares. After this plot I tightened the belt, which made the subsequent plots a lot more accurate.\n\nn_cols &lt;- 12\nn_rows &lt;- 22\n\np &lt;- expand_grid(col = 0:(n_cols-1),\n            row = 0:(n_rows-1),\n            tibble(x = c(-0.5, 0.5,  0.5, -0.5),\n                   y = c( 0.5, 0.5, -0.5, -0.5))) %&gt;%\n  group_by(col, row) %&gt;%\n  mutate(angle = rnorm(1, sd = row/60),\n         xn = x*cos(angle) - y*sin(angle),\n         y = x*sin(angle) + y*cos(angle),\n         x = xn) %&gt;%\n  select(-xn) %&gt;%\n  mutate(x = x + 1 + col,\n         y = y + 1 - row,\n         x = x + rnorm(1)*row/80, # jitter\n         y = y + rnorm(1)*row/60) %&gt;%\n  mutate(xend = lag(x, default = last(x)),\n         yend = lag(y, default = last(y))) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_segment() +\n  coord_fixed() +\n  theme_void()\n\n\n\n\nSchotter, png\n\n\n\n\n\nSchotter, preview\n\n\n\n\n\nSchotter, pen on paper"
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html#eredivisie-clubs",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html#eredivisie-clubs",
    "title": "Closest Eredivisie football club",
    "section": "Eredivisie clubs",
    "text": "Eredivisie clubs\nThe Eredivisie website has a page listing the 18 clubs currently in the league. Every club has its own page (for example, Ajax) which lists various statistics about the club, the players, and the stadium. Using rvest and jsonlite, we collect this list of clubs, and for every club obtain the stadium’s address. As a bonus, we collect the clubs’ logos, which will allow for nice plotting.\nlibrary(rvest)\n\nurl &lt;- \"https://eredivisie.nl/en-us/clubs\"\n\nclub_names &lt;- read_html(url) %&gt;% \n  html_nodes(\".clubs-list__club img\") %&gt;% \n  html_attr(\"title\")\n\n\nget_address &lt;- function(club) {\n  club &lt;- URLencode(club)\n  url &lt;- \"https://eredivisie.nl/en-us/API/DotControl/DCEredivisieLive/Stats/GetThemeSelection?teamname={club}&cid=0&moduleId=1261&tabId=255\"\n\n  str_glue(url) %&gt;% \n    jsonlite::read_json() %&gt;% \n    purrr::pluck(\"club\", \"stadium\")\n}\n\nimg_src &lt;- \"https://eredivisie-images.s3.amazonaws.com/Eredivisie%20images/Eredivisie%20Badges/{teamId}/150x150.png\"\n\nstadiums &lt;- map_dfr(club_names, get_address) %&gt;% \n  mutate(img = str_glue(img_src))\n\nstadiums %&gt;% \n  transmute(teamname, adres, city,\n            img = cell_spec(stringr::str_trunc(img, 40, side = \"left\"),\n                            \"html\", link = img)) %&gt;% \n  head() %&gt;% \n  knitr::kable(escape = FALSE) %&gt;% \n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\"), font_size = 13)\nThe addresses do not yet allow for plotting on a map — for that, we need coordinates. Converting a human-readable address into coordinates (such as latitude and longitude) is called geocoding. There are various APIs that do this. Most of these are paid, but I’ve found a free one that does the job. We provide it with an address as a string, and it gives us back the latitude and longitude. We have to be as specific as possible, to prevent it from finding a location with the same address in another country for example.\ngeo_locate &lt;- function(address) {\n  query_string &lt;- URLencode(address)\n  url &lt;- \"https://api.opencagedata.com/geocode/v1/json?q={query_string}&key=e5580345eb0e46d9a2e17e6a7e2373f7&no_annotations=1&language=nl\"\n\n  str_glue(url) %&gt;% \n    jsonlite::read_json() %&gt;% \n    purrr::pluck(\"results\", 1, \"geometry\")\n}\n\nstadiums &lt;- stadiums %&gt;% \n  mutate(latlon = map(str_glue(\"{adres}, {postalcode} {city}, Nederland\"), geo_locate)) %&gt;% \n  unnest_wider(latlon)\n\nstadiums %&gt;% \n  transmute(teamname, lat, lng,\n            img = cell_spec(stringr::str_trunc(img, 40, side = \"left\"),\n                            \"html\", link = img)) %&gt;% \n  head() %&gt;% \n  knitr::kable(escape = FALSE) %&gt;% \n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\"), font_size = 13)\nNow that we have the clubs’ locations, we can do some data wrangling in order to create the map."
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html#map-of-the-netherlands",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html#map-of-the-netherlands",
    "title": "Closest Eredivisie football club",
    "section": "Map of the Netherlands",
    "text": "Map of the Netherlands\nIn order to plot a map of the Netherlands, we need the geometry of its boundaries. Below, we obtain municipality boundaries from the CBS (the Dutch national statistics bureau). From this, the country boundaries can be obtained with sf::st_union(). Since the CBS data is in a Dutch coordinate system called Rijksdriehoekscoördinaten, we first convert this to WGS84 via st_transform().\n```{r message=FALSE, warning=FALSE} municipalities &lt;- st_read(“https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetFeature&service=WFS&version=2.0.0&typeName=cbs_gemeente_2021_gegeneraliseerd&outputFormat=json”, quiet = TRUE) %&gt;% st_transform(4326) # WGS84\nnl &lt;- st_union(municipalities)\nggplot(municipalities) + geom_sf() + theme_void()\n\n## Voronoi\n\nThe type of map we want to generate is called a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram). We can create a Voronoi diagram from a set of points using `sf::st_voronoi()`. Since that function operates on a `multipoint` and we want the result to be of type `sf`, we do some pre- and post-processing.\n\n```{r warning=FALSE}\nvoronoi &lt;- stadiums %&gt;% \n  select(lng, lat) %&gt;% \n  as.matrix() %&gt;% \n  st_multipoint() %&gt;%\n  st_voronoi(envelope = nl) %&gt;%\n  st_collection_extract() %&gt;% \n  st_set_crs(4326) %&gt;% # WGS84\n  st_sf()\n\nplot(voronoi)\nstadiums %&gt;% \n  select(lng, lat) %&gt;% \n  as.matrix() %&gt;% \n  points()\nNow that we have the Voronoi diagram, we can join it to the stadium data. We intersect it with the boundaries of the Netherlands to make sure the Voronoi lines do not extend to outside the country borders.\n{r message=FALSE, warning=FALSE} stadiums_voronoi &lt;- stadiums %&gt;%   st_as_sf(coords = c(\"lng\", \"lat\"),            crs = 4326, remove = FALSE) %&gt;%    st_join(voronoi, .) %&gt;% # join makes sure we map each tile to the correct centroid   st_cast(\"MULTILINESTRING\") %&gt;% # turn polygons into lines   st_intersection(nl) # within NL"
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html#voronoi",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html#voronoi",
    "title": "Closest Eredivisie football club",
    "section": "Voronoi",
    "text": "Voronoi\nThe type of map we want to generate is called a Voronoi diagram. We can create a Voronoi diagram from a set of points using sf::st_voronoi()."
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html#plotting",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html#plotting",
    "title": "Closest Eredivisie football club",
    "section": "Plotting",
    "text": "Plotting\nFinally, we can plot the map of the Netherlands together with the club logos and the Voronoi edges.\nWe do some manual work to move the club logos away from the points, to make the plot look nicer. The result is a map of the closest Eredivisie football club!\n```{r message=FALSE, warning=FALSE} offsets &lt;- tribble(~ teamname, ~ dlng, ~dlat, “Feyenoord”, 0.12, -0.12, “Sparta Rotterdam”, -0.15, -0.13, “Fortuna Sittard”, -0.15, -0.1, “PSV”, 0.12, 0.1, “Willem II”, 0, -0.15, “RKC Waalwijk”, 0.13, 0.08, “ADO Den Haag”, 0.1, 0.1, “Ajax”, 0.05, 0.14, “FC Utrecht”, 0.1, 0.08, “AZ”, 0.08, 0.1, “Vitesse”, 0.05, -0.15, “FC Twente”, 0.12, 0.1, “Heracles Almelo”, -0.1, -0.15, “PEC Zwolle”, 0, -0.14, “sc Heerenveen”, -0.1, 0.15, “FC Groningen”, -0.15, -0.1, “FC Emmen”, 0.14, 0.1, “VVV-Venlo”, -0.1, 0.1)\nstadiums_voronoi %&gt;% left_join(offsets, by = “teamname”) %&gt;% mutate(lng_img = lng + dlng, lat_img = lat + dlat) %&gt;% ggplot() + geom_sf(data = municipalities, size = 0.1, fill = “#cccccc”) + geom_sf(size = 0.6, fill = NA, color = “black”, alpha = 0.8) + ggimage::geom_image(aes(image = img, x = lng_img, y = lat_img), size = 0.08) + geom_point(aes(x = lng, y = lat)) + theme_void()\n\n## Bonus: interactive map\n\nUsing the `leaflet` library, it is easy to create an interactive map in R. Below, we show the Netherlands with OpenStreetMap, and draw the Voronoi boundaries and markers for the stadiums on top of it. Adding images to leaflet is not straightforward, so we will not add these.\n\n```{r}\nlibrary(leaflet)\n\nstadiums %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"),\n           crs = 4326, remove = FALSE) %&gt;% \n  st_join(voronoi, .) %&gt;% # join makes sure we map each tile to the correct centroid\n  st_cast(\"MULTILINESTRING\") %&gt;% # turn polygons into lines\n  leaflet() %&gt;% \n  setView(lng = 5.6, lat = 52.2, zoom = 7) %&gt;%\n  addTiles() %&gt;% \n  addMarkers(lng = ~lng, lat = ~lat, label = ~teamname) %&gt;% \n  addPolylines()"
  },
  {
    "objectID": "post/2021-02-16-closest-eredivisie-football-club/index.html#bonus-interactive-map",
    "href": "post/2021-02-16-closest-eredivisie-football-club/index.html#bonus-interactive-map",
    "title": "Closest Eredivisie football club",
    "section": "Bonus: interactive map",
    "text": "Bonus: interactive map\nUsing the leaflet library, it is easy to create an interactive map in R."
  },
  {
    "objectID": "post/2021-01-22-riddler-can-you-hunt-for-the-mysterious-numbers/index.html",
    "href": "post/2021-01-22-riddler-can-you-hunt-for-the-mysterious-numbers/index.html",
    "title": "Riddler: Can You Hunt For The Mysterious Numbers?",
    "section": "",
    "text": "This week’s FiveThirtyEight Riddler Classic offers a sudoku-like puzzle:"
  },
  {
    "objectID": "post/2021-01-22-riddler-can-you-hunt-for-the-mysterious-numbers/index.html#linear-programming",
    "href": "post/2021-01-22-riddler-can-you-hunt-for-the-mysterious-numbers/index.html#linear-programming",
    "title": "Riddler: Can You Hunt For The Mysterious Numbers?",
    "section": "Linear programming",
    "text": "Linear programming\nA nice way to solve problems like this is by Integer Linear Programming. Like in my Trees and Tents post, I like to do this in Julia using the JuMP package.\nThe constraints we have are on the products of the numbers in the rows and the columns. Therefore the problem is not linear if we implement it naively (a variable for every cell denoting the number between 1 and 9 it should contain). We need to do something a little bit more clever. A natural way to look at this problem is via the prime factorizations of the row and the column products. There are four primes between 1 and 9, namely 2, 3, 5 and 7. Given the prime factorization of every row and column, we only need to distribute these primes over the cells. Since 294 = 2^1\\cdot 3^1\\cdot 5^0 \\cdot 7^2, we know that in the first row there should be a 2, a 3, no 5 and two 7s. Note that a cell can contain multiple primes, or none at all (meaning it is a 1).\nWe therefore need a variable for every cell and each of the 4 possible primes, the value of which denotes the multiplicity of this prime in this cell.\nIn Julia, we first import the required packages, and define the row and column products.\nusing JuMP, GLPK\nusing Primes: factor, primes\nusing DataStructures: DefaultDict\n\nrow_prods = [294, 216, 135, 98, 112, 84, 245, 40];\ncol_prods = [8_890_560, 156_800, 55_566];\nprms = primes(1, 9);\n\nn_prms = length(prms);\nn_rows = length(row_prods);\nn_cols = length(col_prods);\nNow, we create lists of the multiplicities of the four primes in the rows and the columns. The [1, 1, 0, 2] denotes the multiplicities of 2, 3, 5 and 7 in the first row.\nrow_factors = factor.(Dict, row_prods);\ncol_factors = factor.(Dict, col_prods);\n\nrow_factors = [[DefaultDict(0, row_factors[row])[prms[i]] for i in 1:4] for row in 1:n_rows]\ncol_factors = [[DefaultDict(0, col_factors[col])[prms[i]] for i in 1:4] for col in 1:n_cols]\n8-element Array{Array{Int64,1},1}:\n [1, 1, 0, 2]\n [3, 3, 0, 0]\n [0, 3, 1, 0]\n [1, 0, 0, 2]\n [4, 0, 0, 1]\n [2, 1, 0, 1]\n [0, 0, 1, 2]\n [3, 0, 1, 0]\n3-element Array{Array{Int64,1},1}:\n [6, 4, 1, 3]\n [7, 0, 2, 2]\n [1, 4, 0, 3]\nNow onto the actual problem definition. As stated, we need a variable for every cell and every prime. Every prime can occur 0 or more times, and its multiplicity is an integer.\nmodel = Model(with_optimizer(GLPK.Optimizer))\n\n@variable(model, 0 &lt;= x[1:n_rows, 1:n_cols, 1:n_prms], integer=true)\nRows and column prime multiplicities should add up to the correct numbers.\nfor i in 1:n_rows, p in 1:n_prms\n    @constraint(model, sum(x[i, :, p]) == row_factors[i][p])\nend\n\nfor j in 1:n_cols, p in 1:n_prms\n    @constraint(model, sum(x[:, j, p]) == col_factors[j][p])\nend\nFinally, we want every cell to be an integer between 1 and 9, which means 2^{p_2}\\cdot 3^{p_3}\\cdot 5^{p_5}\\cdot 7^{p_7} \\le 9, or p_2\\log(2) + p_3\\log(3) + p_5\\log(5)+p_7\\log(7) \\le \\log(9), a constraint which is linear in its variables.\nfor i in 1:n_rows, j in 1:n_cols\n    @constraint(model, sum(x[i, j, :] .* log.(prms)) &lt;= log(9))\nend\nAnd that is all! We let JuMP do the hard work and come up with a feasible solution. As an example, we print out the number of times a 2 (the first prime) appears in every cell.\nJuMP.optimize!(model)\n\nsolution = JuMP.value.(x);\n\nsolution[:, :, 1]\n8×3 Array{Float64,2}:\n 0.0  0.0  1.0\n 0.0  3.0  0.0\n 0.0  0.0  0.0\n 0.0  1.0  0.0\n 3.0  1.0  0.0\n 0.0  2.0  0.0\n 0.0  0.0  0.0\n 3.0  0.0  0.0\nThe final solution to the riddle is then the product of every prime to the power of its multiplicity:\n[[Int(prod(prms .^ solution[i, j, :])) for j in 1:n_cols] for i in 1:n_rows]\n8-element Array{Array{Int64,1},1}:\n [7, 7, 6]\n [9, 8, 3]\n [9, 5, 3]\n [7, 2, 7]\n [8, 2, 7]\n [7, 4, 3]\n [5, 7, 7]\n [8, 5, 1]\nAnd there’s the solution! I love how easy it is to have JuMP solve such a problem after coming up with a way to phrase it as a linear program."
  },
  {
    "objectID": "post/2018-09-20-image-approximation-with-keras-in-r/index.html",
    "href": "post/2018-09-20-image-approximation-with-keras-in-r/index.html",
    "title": "Learning image representation with Keras in R",
    "section": "",
    "text": "An image can be viewed as a function mapping pixel locations x,y to color values R,G,B. Since neural networks are function approximators, we can train such a network to approximate an image. The network is then a representation of the image as a function and its contents can be displayed by evaluating the network for all pixel pairs x,y.\nThe Keras package for R is now approximately 1 year old but I have to admit that I usually go to Python to implement neural networks. Since I wanted to try the imager package for R for a while, let’s hit two birds with one stone and do this exercise in R.\nFirst, we need the right packages.\nlibrary(tidyverse)\nlibrary(keras)\nlibrary(imager)\nTo honour the name of this blog, let’s pick an image of the bird species godwit (scientific name: limosa) and load it using imager’s load.image.\n# Image from Pixabay\nim_url &lt;- \"https://cdn.pixabay.com/photo/2015/09/18/00/13/bar-tailed-godwit-944883_640.jpg\"\nim &lt;- load.image(im_url)\ndim(im)\n## [1] 640 480   1   3\nThe image is 640x480 pixels in size, consists of 1 frame (it is not a video) and 3 color channels.\nPlotting the image is easy. We choose to show the image without axes and make the margins small for aestetic reasons.\nplot(im, axes = FALSE)\n\n\n\nA godwit. Source: pixabay.com\n\n\nWe convert the image to a data frame by calling as.data.frame on the image object. This data frame by default has 4 columns: an x and y column to identify each pixel’s location, a color channel column cc, and the value this channel takes at this pixel. We spread the key-value pairs in the cc and value columns into columns cc_1, cc_2, cc_3 representing the three color values.\ndf &lt;- im %&gt;% as.data.frame %&gt;% spread(cc, value)\nhead(df)\n##   x y         1         2         3\n## 1 1 1 0.8117647 0.8941176 0.9607843\n## 2 1 2 0.8117647 0.8941176 0.9607843\n## 3 1 3 0.8117647 0.8941176 0.9607843\n## 4 1 4 0.8117647 0.8980392 0.9529412\n## 5 1 5 0.8196078 0.8941176 0.9529412\n## 6 1 6 0.8117647 0.8862745 0.9450980\nThe color values are between 0 (absent) and 1 (present).\nWe create input and output matrices which can be fed into keras.\nX &lt;- as.matrix(select(df, x, y))\nY &lt;- as.matrix(select(df, -x, -y))\nCreating a neural network with keras is very easy. We tell it that we want an ‘ordinary’ feed forward neural network with keras_model_sequential(). For starters, we create a one-layer network with 2 input nodes (x and y coordinates) and 3 output nodes (R, G, B values). We choose a ‘sigmoid’ activation function because we want the output values to be between 0 and 1.\nmodel &lt;- keras_model_sequential() \nmodel %&gt;%\n  layer_dense(3, input_shape = 2, activation = \"sigmoid\")\nsummary(model)\n## ___________________________________________________________________________\n## Layer (type)                     Output Shape                  Param #     \n## ===========================================================================\n## dense_1 (Dense)                  (None, 3)                     9           \n## ===========================================================================\n## Total params: 9\n## Trainable params: 9\n## Non-trainable params: 0\n## ___________________________________________________________________________\nWhen compiling the model, we tell it what loss function we need, what optimizer and what kind of metrics we want it to display while training. We choose a mean_squared_error loss, which means that we penalize every color channel equally, taking the square of the error for every channel and every pixel. We optimize the network using the widely used Adam optimizer and we want to see its accuracy while training.\nmodel %&gt;% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\nNext, we actually fit the model, giving it the expected input and output matrices X and Y. We set the batch size to 128 such that training is faster than the default 32. We train the network for 20 epochs.\nmodel %&gt;% fit(\n  X, Y, \n  batch_size = 128,\n  epochs = 20\n)\n\n\n\nTraining progress\n\n\nTraining takes about 1 minute, and as can be seen from the training plot, it seems that both the loss and the accuracy have saturated. An accuracy of 0.9 sounds good, but let’s take a look at the output image before we get too happy.\nFirst, we need to recreate the original dataframe with the predicted pixel values. For every (x,y) pair in our matrix X we need to make a prediction. We duplicate the original dataframe fill the color channel columns with the network’s output values. We then call gather on this dataframe to create the cc and value columns necessary for the imager library.\ndf_out &lt;- df\ndf_out[, 3:5] &lt;- model %&gt;% predict(X)\ndf_out &lt;- df_out %&gt;%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\nWith the data frame in the right form, we can display the result next to the original image very easily. We convert the data frame to an image with the as.cimg function and paste it next to the original image by wrapping them inside a list and calling imappend on it, specifying we want them appended on the horizontal axis.\nas.cimg(df_out) %&gt;% list(im, .) %&gt;%\n  imappend(\"x\") %&gt;% plot(axes = F)\n\n\n\nThe network learned a gradient\n\n\nThe result is a vertical gradient, which can also be seen when looking at the model weights.\nmodel$get_weights()\n## [[1]]\n##              [,1]         [,2]         [,3]\n## [1,]  0.000755751  0.001207316  0.001467007\n## [2,] -0.002216082 -0.004577803 -0.006758745\n## \n## [[2]]\n## [1] 1.460977 2.029516 2.619324\nThe values corresponding to the x column are positive, while the y values are negative and about one order of magnitute larger.\n\nMore layers\nNow let’s try a network with more layers. More specifically, let’s add two layers with both 10 nodes followed by sigmoid activations.\nmodel &lt;- keras_model_sequential()\nmodel %&gt;%\n  layer_dense(10, input_shape = 2, activation = \"sigmoid\") %&gt;%\n  layer_dense(10, activation = \"sigmoid\") %&gt;%\n  layer_dense(3, activation = \"sigmoid\")\n\nmodel %&gt;% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\nmodel %&gt;% fit(\n  X, Y,\n  batch_size = 128,\n  epochs = 20\n)\n\ndf_out &lt;- df\ndf_out[, 3:5] &lt;- model %&gt;% predict(X)\ndf_out &lt;- df_out %&gt;%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\n\nas.cimg(df_out) %&gt;% list(im, .) %&gt;%\n  imappend(\"x\") %&gt;% plot(axes = F)\n\n\n\nSome features of the bird appear\n\n\nAs can be seen, the image is already picking up some countours of the bird, together with some ‘rays’ coming from the top-left corner. If we go for an even deeper network, more of the bird’s features can be recognized from the image.\nmodel &lt;- keras_model_sequential()\nmodel %&gt;%\n  layer_dense(100, input_shape = 2, activation = \"tanh\") %&gt;%\n  layer_dense(100, activation = \"relu\") %&gt;%\n  layer_dense(100, activation = \"relu\") %&gt;%\n  layer_dense(100, activation = \"relu\") %&gt;%\n  layer_dense(100, activation = \"relu\") %&gt;%\n  layer_dense(3) %&gt;% \n  layer_activation_relu(max_value=1)\n\nmodel %&gt;% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\nmodel %&gt;% fit(\n  X, Y,\n  batch_size = 128,\n  epochs = 100\n)\n\ndf_out &lt;- df\ndf_out[, 3:5] &lt;- model %&gt;% predict(X)\ndf_out &lt;- df_out %&gt;%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\n\nas.cimg(df_out) %&gt;% list(im, .) %&gt;%\n  imappend(\"x\") %&gt;% plot(axes=F)\n\n\n\nThe image is nicely recovered\n\n\nWays to improve this might include more layers, more epochs, different activation functions, different loss function. For now, I’m really happy with the result!"
  },
  {
    "objectID": "post/2019-04-21-trees-and-tents/index.html",
    "href": "post/2019-04-21-trees-and-tents/index.html",
    "title": "Trees and Tents",
    "section": "",
    "text": "Trees and Tents is a logic game in which tents need to be placed in a grid. Every tent must be next to a tree, and tents cannot touch horizontally, vertically or diagonally. For every row and column, the number of tents is given.\nThe website https://brainbashers.com/tents.asp posts a new puzzle every day, of which a 12x12 example is the following:\nIn this post, I will implement a solver for this puzzle in Julia using the JuMP package for mathematical optimization.\nThe complete code as a jupyter notebook can be found here."
  },
  {
    "objectID": "post/2019-04-21-trees-and-tents/index.html#problem-definition",
    "href": "post/2019-04-21-trees-and-tents/index.html#problem-definition",
    "title": "Trees and Tents",
    "section": "Problem definition",
    "text": "Problem definition\nThe puzzle can be formulated as a linear programming problem. There are three types of constraints:\n\nevery tent must be next to a tree,\nthe number of tents for every row and column is given,\ntents are not allowed to touch.\n\nEven though we treat this puzzle as an optimization problem, there is no objective to be maximized or minimized in this case; any placements of tents satisfying the constraints is a valid solution. This is similar to the sudoku solutions in which the objective is also unimportant.\nJulia’s JuMP package provides a clean interface to defining such a problem, and solving it with various solvers. In this case, we use GLPK since it’s free and works great for this problem. Solving the problem is a matter of writing down the constraints and calling JuMP.optimize.\nusing JuMP, GLPK\nAfter loading the necessary packages, we need to define the puzzle displayed above. In the website’s html code, the puzzle is represented by three strings.\n\nlcpuzzle is a string of numbers, denoting the cells from left to right, top to bottom. The values represent whether the corresponding cell is blank (0), or contains a tree (1) or a tent (2). Note that this string contains the solution to the problem as well, so this allows us to check the solution that JuMP found afterwards.\nlcrownumbers is a string of numbers which denote the number of tents per row.\nlcrolnumbers is a string of numbers which denote the number of tents per column.\n\n# from html, 0=blank, 1=tree, 2=tent\nlcpuzzle = \"001202011200021001020012000120001200020100100000010200200000201000001212102020000000000010021012201000000200102002020102000001010001001201201212\"\nlcrownumbers = \"332123222404\"\nlccolnumbers = \"222322230505\"\nWe rewrite the above representation into something that is more easy to work with:\n\ntrees is a list of (i,j) tuples corresponding to the locations of the trees.\nrow_sums and col_sums are lists of integers denoting the totals per row and column.\nn is the size of the game, which is in this case 12.\n\npuzzle = [parse(Int, i) for i in lcpuzzle]\nn = Int(sqrt(length(puzzle)))\n\ntrees = [(i, j) for i in 1:n, j in 1:n if puzzle[(i-1)*n + j] == 1]\nrow_sums = [parse(Int, i) for i in lcrownumbers]\ncol_sums = [parse(Int, i) for i in lccolnumbers]\nWe initialize the solver, telling JuMP that we want to use GLPK. The variable that JuMP optimizes over is an n\\times n matrix x, consisting of binary values, where a 0 means no tent for that cell, and a 1 means that the cell contains a tent.\nmodel = Model(with_optimizer(GLPK.Optimizer))\n\n@variable(model, x[1:n, 1:n], Bin)"
  },
  {
    "objectID": "post/2019-04-21-trees-and-tents/index.html#constraints",
    "href": "post/2019-04-21-trees-and-tents/index.html#constraints",
    "title": "Trees and Tents",
    "section": "Constraints",
    "text": "Constraints\nWe are now ready to define the constraints. We start with the requirement that a tent be next to a tree.\nFirst, a tent cannot be on top of a tree. Hence, if a tree is on position (row, col), then x[row, col] must be 0.\nNext, for every cell (row, col) we check whether there is a tree next to this cell. If not, then this cell cannot contain a tent, hence x[row, col] should be 0.\nfor row in 1:n, col in 1:n\n    # is not on a tree\n    if (row, col) in trees\n        @constraint(model, x[row, col] == 0)\n    end\n    \n    # should be next to a tree \n    found = false\n    for tree in trees\n        if abs(tree[1] - row) + abs(tree[2] - col) &lt;= 1\n            found = true\n        end\n    end\n    if !found\n        @constraint(model, x[row, col] == 0)\n    end\nend\nThe next constraint makes sure that the number of tents in the rows and columns add up to the right values. This can be done simply by summing the columns and rows of the matrix x, since this equals counting the number of ones.\nfor i in 1:n\n    @constraint(model, sum(x[i, :]) == row_sums[i])\n    @constraint(model, sum(x[:, i]) == col_sums[i])\nend\nLastly, we need to encode the fact that the tents may not touch horizontally, vertically or diagonally. In other words, every 2\\times2 square contains at most one tree, which is also easy to establish using the representation we chose.\nfor row in 1:(n-1), col in 1:(n-1)\n    @constraint(model, sum(x[row:row+1, col:col+1]) &lt;= 1)\nend"
  },
  {
    "objectID": "post/2019-04-21-trees-and-tents/index.html#solution",
    "href": "post/2019-04-21-trees-and-tents/index.html#solution",
    "title": "Trees and Tents",
    "section": "Solution",
    "text": "Solution\nNow that we have represented the puzzle as a Linear Program, we can simply tel JuMP to optimize this. Within a second, it spits out a solution. Since Julia supports Unicode characters, we can visualize the solution as a grid of emoji.\nJuMP.optimize!(model)\n\nsolution = JuMP.value.(x)\n\nfor row in 1:n\n    for col in 1:n\n        if solution[row, col] == 1\n            print(\"⛺\")\n        elseif (row, col) in trees\n            print(\"🌲\")\n        else\n            print(\"◻️\")\n        end\n        print(\" \")\n    end\n    println(\"\")\nend\n◻️ ◻️ 🌲 ⛺ ◻️ ⛺ ◻️ 🌲 🌲 ⛺ ◻️ ◻️ \n◻️ ⛺ 🌲 ◻️ ◻️ 🌲 ◻️ ⛺ ◻️ ◻️ 🌲 ⛺ \n◻️ ◻️ ◻️ 🌲 ⛺ ◻️ ◻️ ◻️ 🌲 ⛺ ◻️ ◻️ \n◻️ ⛺ ◻️ 🌲 ◻️ ◻️ 🌲 ◻️ ◻️ ◻️ ◻️ ◻️ \n◻️ 🌲 ◻️ ⛺ ◻️ ◻️ ⛺ ◻️ ◻️ ◻️ ◻️ ◻️ \n⛺ ◻️ 🌲 ◻️ ◻️ ◻️ ◻️ ◻️ 🌲 ⛺ 🌲 ⛺ \n🌲 ◻️ ⛺ ◻️ ⛺ ◻️ ◻️ ◻️ ◻️ ◻️ ◻️ ◻️ \n◻️ ◻️ ◻️ ◻️ 🌲 ◻️ ◻️ ⛺ 🌲 ◻️ 🌲 ⛺ \n⛺ ◻️ 🌲 ◻️ ◻️ ◻️ ◻️ ◻️ ◻️ ⛺ ◻️ ◻️ \n🌲 ◻️ ⛺ ◻️ ◻️ ⛺ ◻️ ⛺ ◻️ 🌲 ◻️ ⛺ \n◻️ ◻️ ◻️ ◻️ ◻️ 🌲 ◻️ 🌲 ◻️ ◻️ ◻️ 🌲 \n◻️ ◻️ 🌲 ⛺ ◻️ 🌲 ⛺ ◻️ 🌲 ⛺ 🌲 ⛺ \nThat’s it! I like the JuMP syntax a lot and will definitely use it to solve more puzzles!"
  }
]