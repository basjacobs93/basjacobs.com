{"title":"Physics-informed neural network for population dynamics","markdown":{"yaml":{"title":"Physics-informed neural network for population dynamics","author":"Bas","date":"2024-02-09","math":true},"headingText":"Packages","containsRefs":false,"markdown":"\n\nA Physics-Informed Neural Network (PINN) is a neural network that incorporates knowledge about physical laws alongside data. These physical (or chemical, or biological) laws can be incorporated into the network in the form of differential equations. Among other things, PINNs can be used to estimate parameters of a differential equation based on observational data.  \n\nLast week, Dr. Riccardo Taormina gave a guest lecture in my organization in which he explained the workings and advantages of PINNs. In the accompanying workshop he used synthetic data to show that it is possible to estimate the parameters of differential equations of a harmonic oscillator and an advection–diffusion process.  \n\n\n![Lynx and snowshoe hare (photo by Tom and Pat Leeson)](https://www2.nau.edu/lrm22/lessons/predator_prey/hare-lynx.jpg)\n\nI once read [an article by Bob Carpenter](https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html) about predator-prey population dynamics of hares and lynxes in Canada. The interesting thing of this article is that it uses actual observations rather than synthetical data. The author uses data for the number of pelts of these hares and lynxes obtained by Hudson’s Bay Company in the years 1900-1920 to estimate the parameters of a [Lotka-Volterra model](https://en.wikipedia.org/wiki/Lotka–Volterra_equations). He uses the statistical modeling software [Stan](https://mc-stan.org) to generate these estimates based on Markov-Chain Monte-Carlo. This gives posterior estimates (distributions), which elegantly account for measurement and estimation uncertainty. \nIn the [Joseph M. Mahaffy course Mathematical Modeling](https://jmahaffy.sdsu.edu/courses/f09/math636/lectures/lotka/qualde2.html), the author uses an optimization routine to estimate the parameters of the Lotka-Volterra equations using the same dataset.  \n\nIn this post, I will show that PINNs can also be used to estimate those parameters. Let's get started!\n\nWe will use the Python language with packages Pandas to load the data, Torch to train the neural network and M:w\nplotlib for plotting.\n\n```python\n%matplotlib inline\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom matplotlib import pyplot as plt\n```\n\n## The data\n\nThe data can be downloaded from various places, all derived from http://www.math.tamu.edu/~phoward/m442/modbasics.pdf. I load it from [here](https://raw.githubusercontent.com/cas-bioinf/statistical-simulations/master/hudson-bay-lynx-hare.csv). The dataset consists of observations of lynxes and Hare pelts (in thousands) across the years 1900 to 1920.\n\n\n```python\nurl = \"https://raw.githubusercontent.com/cas-bioinf/statistical-simulations/master/hudson-bay-lynx-hare.csv\"\ndf = pd.read_csv(url, sep=\",\\\\s\", skiprows=2, index_col=0)\ndf\n```\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Lynx</th>\n      <th>Hare</th>\n    </tr>\n    <tr>\n      <th>Year</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1900</th>\n      <td>4.0</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>1901</th>\n      <td>6.1</td>\n      <td>47.2</td>\n    </tr>\n    <tr>\n      <th>1902</th>\n      <td>9.8</td>\n      <td>70.2</td>\n    </tr>\n    <tr>\n      <th>1903</th>\n      <td>35.2</td>\n      <td>77.4</td>\n    </tr>\n    <tr>\n      <th>1904</th>\n      <td>59.4</td>\n      <td>36.3</td>\n    </tr>\n    <tr>\n      <th>1905</th>\n      <td>41.7</td>\n      <td>20.6</td>\n    </tr>\n    <tr>\n      <th>1906</th>\n      <td>19.0</td>\n      <td>18.1</td>\n    </tr>\n    <tr>\n      <th>1907</th>\n      <td>13.0</td>\n      <td>21.4</td>\n    </tr>\n    <tr>\n      <th>1908</th>\n      <td>8.3</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>1909</th>\n      <td>9.1</td>\n      <td>25.4</td>\n    </tr>\n    <tr>\n      <th>1910</th>\n      <td>7.4</td>\n      <td>27.1</td>\n    </tr>\n    <tr>\n      <th>1911</th>\n      <td>8.0</td>\n      <td>40.3</td>\n    </tr>\n    <tr>\n      <th>1912</th>\n      <td>12.3</td>\n      <td>57.0</td>\n    </tr>\n    <tr>\n      <th>1913</th>\n      <td>19.5</td>\n      <td>76.6</td>\n    </tr>\n    <tr>\n      <th>1914</th>\n      <td>45.7</td>\n      <td>52.3</td>\n    </tr>\n    <tr>\n      <th>1915</th>\n      <td>51.1</td>\n      <td>19.5</td>\n    </tr>\n    <tr>\n      <th>1916</th>\n      <td>29.7</td>\n      <td>11.2</td>\n    </tr>\n    <tr>\n      <th>1917</th>\n      <td>15.8</td>\n      <td>7.6</td>\n    </tr>\n    <tr>\n      <th>1918</th>\n      <td>9.7</td>\n      <td>14.6</td>\n    </tr>\n    <tr>\n      <th>1919</th>\n      <td>10.1</td>\n      <td>16.2</td>\n    </tr>\n    <tr>\n      <th>1920</th>\n      <td>8.6</td>\n      <td>24.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nThe data follow a cyclical pattern in which increases in the population of hares are followed by increases in the lynx populations in the subsequent years.\n\n\n```python\ndf[[\"Hare\", \"Lynx\"]].plot(figsize=(10,6), grid=True, xticks = df.index.astype(int)[::2], title=\"Hudson Bay Lynx-Hare Dataset\", ylabel=\"Number of pelts (thousands)\")\n```   \n![](images/lynx_5_1.png){width=600px}\n\n## The Lotka-Volterra model\n\nThe Lotka-Volterra population model the change of predator and pray populations over time:\n - $u(t)\\ge0$ is the population size of the prey species at time t (the hare in our case), and\n - $v(t)\\ge0$ is the population size of the predator species (the lynx in our case).\n\nBy using such a model, some assumptions are made on the dynamics of these populations. We will not go into those here, but they can be found on the [Wikipedia page](https://en.wikipedia.org/wiki/Lotka–Volterra_equations).  \nThe population sizes over times of the two species are modelled in terms of four parameters, $\\alpha,\\beta,\\gamma,\\delta\\ge0$ as\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} u = \\alpha u - \\beta u v\n$$\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} v = \\delta uv - \\gamma v\n$$\n\nIt is these four parameters we will estimate in this post. They have the following interpretations:\n - $\\alpha$ is the growth rate of the prey population,\n - $\\beta$ is the rate of shrinkage relative to the product of the population sizes,\n - $\\gamma$ is the shrinkage rate of the predator population,\n - $\\delta$ is the growth rate of the predator population as a factor of the product of the population sizes.\n\n\nIn the Stan article, the author obtains posterior mean point estimates\n\n$$\n\\hat{\\alpha} = 0.55,\\quad\n\\hat{\\beta} = 0.028,\\quad\n\\hat{\\gamma} = 0.80,\\quad\n\\hat{\\delta} = 0.024,\n$$\nand in the Mathematical Modeling course, the author obtains\n$$\n\\alpha^* = 0.55,\\quad\n\\beta^* = 0.028,\\quad\n\\gamma^* = 0.84,\\quad\n\\delta^* = 0.026.\n$$\n\n\nTo begin, we first set up some plotting functionality for keeping track of the training progress.\n\n\n```python\ndef plot_result(i, t, y, yh, loss, xp=None, lossp=None, params=None):\n    plt.figure(figsize=(8, 4))\n    plt.xlim(0, 40)\n    plt.ylim(0, 85)\n\n    # Data\n    plt.plot(t, y, \"o-\", linewidth=3, alpha=0.2, label=[\"Exact hare\", \"Exact lynx\"])\n    plt.gca().set_prop_cycle(None)  # reset the colors\n    plt.plot(t, yh, linewidth=1, label=[\"Prediction hare\", \"Prediction lynx\"])\n\n    # Physics training ticks\n    if xp is not None:\n        plt.scatter(xp, 0*torch.ones_like(xp), s=3, color=\"tab:green\", alpha=1, marker=\"|\", label=\"Physics loss timesteps\")\n\n    # Legend\n    l = plt.legend(loc=(0.56, 0.1), frameon=False, fontsize=\"large\")\n    plt.setp(l.get_texts(), color=\"k\")\n\n    # Title\n    plt.text(23, 73, f\"Training step: {i}\", fontsize=\"xx-large\", color=\"k\")\n    rmse_text = f\"RMSE data: {int(torch.round(loss))}\"\n    if lossp is not None:\n        rmse_text += f\", physics: {int(torch.round(lossp))}\"\n    plt.text(23, 66, rmse_text, fontsize=\"medium\", color=\"k\")\n    if params is not None:\n        plt.text(23, 60, params, fontsize=\"medium\", color=\"k\")\n\n    plt.axis(\"off\")\n    plt.show()\n```\n\nThe inputs for the model are the timesteps $t=0..20$ corresponding to the years 1900-1920 and the outputs are the numbers of hares and lynxes.\n\n\n```python\ndf[\"Timestep\"] = df.index - min(df.index)\nt = torch.tensor(df[\"Timestep\"].values, dtype=torch.float32).view(-1,1)\ny = torch.tensor(df[[\"Hare\", \"Lynx\"]].values, dtype=torch.float32)\n```\n\nFirst, we define a simple dense feed-forward neural network using Torch.\n\n\n```python\nclass NN(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_layers):\n        super().__init__()\n        self.encode = nn.Sequential(nn.Linear(n_input, n_hidden), nn.Tanh())\n        self.hidden = nn.Sequential(*[nn.Sequential(nn.Linear(n_hidden, n_hidden), nn.Tanh()) for _ in range(n_layers-1)])\n        self.decode = nn.Linear(n_hidden, n_output)\n        \n    def forward(self, x):\n        x = self.encode(x)\n        x = self.hidden(x)\n        x = self.decode(x)\n        return x\n```\n\nWe can train neural network on the dataset (predicting the number of lynx and hares based on timestep $t$) and see that it is complex enough to be able to fit the dataset well. This takes a couple of seconds on my machine.\n\n\n```python\ntorch.manual_seed(123)\nmodel = NN(1, 2, 8, 2)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nfor i in range(50000):\n    optimizer.zero_grad()\n\n    yh = model(t)\n    loss = torch.mean((yh - y)**2)\n\n    loss.backward()\n    optimizer.step()\n    if i%10000 == 0:\n        yh = model(t).detach()\n        \n        plot_result(i, t, y, yh, loss, None)\n```\n\n\n    \n![](images/lynx_14_0.png)\n    \n\n\n\n    \n![](images/lynx_14_1.png)\n    \n\n\n\n    \n![](images/lynx_14_2.png)\n    \n\n\n\n    \n![](images/lynx_14_3.png)\n    \n\n\n\n    \n![](images/lynx_14_4.png)\n    \n\n\nNext, we use exactly the same architecture but add parameters `alpha`, `beta`, `gamma` and `delta` reflecting the parameters in the Lotka-Volterra equations. We initialize them randomly between 0 and 1, and add them to the list of model paramters so that they are updated in the backpropagation steps.  \n\n\n```python\ntorch.manual_seed(123)\nmodel = NN(1, 2, 8, 2)\n\nalpha = torch.rand(1, requires_grad=True)\nbeta = torch.rand(1, requires_grad=True)\ngamma = torch.rand(1, requires_grad=True)\ndelta = torch.rand(1, requires_grad=True)\n\nextra_parameters = [alpha, beta, gamma, delta]\nparameters = list(model.parameters()) + extra_parameters\noptimizer = torch.optim.Adam(parameters, lr=1e-3)\n```\n\nWe define `t_physics`; the range of timesteps on which we numerically evaluate the differential equations. In this case, we evaluate them every 0.1 year.\n\n\n```python\nt_physics = torch.arange(0, len(t), 0.1, dtype=torch.float32).view(-1,1).requires_grad_(True)\n```\n\nTraining the PINN is identical to training the normal neural network, except that the loss function consists of a combination of the data loss and the physics loss.  \nThe data loss is identical to the loss of the previous neural network.  \nFor the physics loss, note that we can rewrite the Lotka-Volterra equations as\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} u - (\\alpha u - \\beta u v) = 0,\n$$\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} v - (\\delta uv - \\gamma v) = 0.\n$$\nWe calculate the mean of the square of the left-hand side across the values of $t$ in `t_physics`, thus punishing the model if the equations are far away from 0. We use `torch.autograd` to estimate the partial derivatives, which means that these partial derivatives are based on what the neural network has learned so far.  \nWe calculate a weighted sum of the physics and data loss, using a weight of `lambda1` set to 0.01. The value of this weight can matter quite a lot and depends on the relative sizes of the errors. Intuitively, it captures how much you trust the data versus the physical laws.\n\n\n```python\nlambda1 = 1e-2\nfor i in range(50000):\n    optimizer.zero_grad()\n\n    # Data loss\n    yh = model(t)\n    data_loss = torch.mean((yh - y)**2)\n\n    # Physics loss\n    yhp = model(t_physics)  # output of the model at the t_physics timesteps\n    u, v = yhp[:, 0], yhp[:, 1]  # hare and lynx populations according to the neural network\n    dudt  = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0].flatten() # time derivative of hare\n    dvdt = torch.autograd.grad(v,  t_physics, torch.ones_like(v),  create_graph=True)[0].flatten() # time derivative of lynx\n    dudt_loss = torch.mean((dudt - (alpha*u - beta*u*v))**2)\n    dvdt_loss = torch.mean((dvdt - (delta*u*v - gamma*v))**2)\n    physics_loss = dudt_loss + dvdt_loss\n\n    loss = data_loss + lambda1*physics_loss\n    loss.backward()\n    optimizer.step()\n\n    if i%10000 == 0:\n        yh = model(t).detach()\n        tp = t_physics.detach()\n        \n        a, b, c, d = [round(param.item(), 3) for param in extra_parameters]\n        params = rf\"$\\alpha={a}, \\beta={b}, \\gamma={c}, \\delta={d}$\"\n        plot_result(i, t, y, yh, loss, tp, physics_loss, params)\n```\n\n\n    \n![](images/lynx_20_0.png)\n    \n\n\n    \n![](images/lynx_20_1.png)\n    \n\n\n\n    \n![](images/lynx_20_2.png)\n    \n\n\n\n    \n![](images/lynx_20_3.png)\n    \n\n\n\n    \n![](images/lynx_20_4.png)\n    \n\n\nWe see that the model is still able to fit well (albeit a little bit worse), and is able to estimate values for the parameters. Specifically, it estimates\n\n$$\n\\hat{\\alpha} = 0.57,\\quad\n\\hat{\\beta} = 0.027,\\quad\n\\hat{\\gamma} = 0.94,\\quad\n\\hat{\\delta} = 0.026,\n$$\nwhich are very close to the results of the other two approaches mentioned above. Only the estimate for $\\gamma$ deviates significantly from the other estimates.  \n\nHowever, this approach is very simple to implement and requires no knowledge about the differential equations or Bayesian modelling software.  \n\nThanks for reading!\n","srcMarkdownNoYaml":"\n\nA Physics-Informed Neural Network (PINN) is a neural network that incorporates knowledge about physical laws alongside data. These physical (or chemical, or biological) laws can be incorporated into the network in the form of differential equations. Among other things, PINNs can be used to estimate parameters of a differential equation based on observational data.  \n\nLast week, Dr. Riccardo Taormina gave a guest lecture in my organization in which he explained the workings and advantages of PINNs. In the accompanying workshop he used synthetic data to show that it is possible to estimate the parameters of differential equations of a harmonic oscillator and an advection–diffusion process.  \n\n\n![Lynx and snowshoe hare (photo by Tom and Pat Leeson)](https://www2.nau.edu/lrm22/lessons/predator_prey/hare-lynx.jpg)\n\nI once read [an article by Bob Carpenter](https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html) about predator-prey population dynamics of hares and lynxes in Canada. The interesting thing of this article is that it uses actual observations rather than synthetical data. The author uses data for the number of pelts of these hares and lynxes obtained by Hudson’s Bay Company in the years 1900-1920 to estimate the parameters of a [Lotka-Volterra model](https://en.wikipedia.org/wiki/Lotka–Volterra_equations). He uses the statistical modeling software [Stan](https://mc-stan.org) to generate these estimates based on Markov-Chain Monte-Carlo. This gives posterior estimates (distributions), which elegantly account for measurement and estimation uncertainty. \nIn the [Joseph M. Mahaffy course Mathematical Modeling](https://jmahaffy.sdsu.edu/courses/f09/math636/lectures/lotka/qualde2.html), the author uses an optimization routine to estimate the parameters of the Lotka-Volterra equations using the same dataset.  \n\nIn this post, I will show that PINNs can also be used to estimate those parameters. Let's get started!\n\n## Packages\nWe will use the Python language with packages Pandas to load the data, Torch to train the neural network and M:w\nplotlib for plotting.\n\n```python\n%matplotlib inline\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom matplotlib import pyplot as plt\n```\n\n## The data\n\nThe data can be downloaded from various places, all derived from http://www.math.tamu.edu/~phoward/m442/modbasics.pdf. I load it from [here](https://raw.githubusercontent.com/cas-bioinf/statistical-simulations/master/hudson-bay-lynx-hare.csv). The dataset consists of observations of lynxes and Hare pelts (in thousands) across the years 1900 to 1920.\n\n\n```python\nurl = \"https://raw.githubusercontent.com/cas-bioinf/statistical-simulations/master/hudson-bay-lynx-hare.csv\"\ndf = pd.read_csv(url, sep=\",\\\\s\", skiprows=2, index_col=0)\ndf\n```\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Lynx</th>\n      <th>Hare</th>\n    </tr>\n    <tr>\n      <th>Year</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1900</th>\n      <td>4.0</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>1901</th>\n      <td>6.1</td>\n      <td>47.2</td>\n    </tr>\n    <tr>\n      <th>1902</th>\n      <td>9.8</td>\n      <td>70.2</td>\n    </tr>\n    <tr>\n      <th>1903</th>\n      <td>35.2</td>\n      <td>77.4</td>\n    </tr>\n    <tr>\n      <th>1904</th>\n      <td>59.4</td>\n      <td>36.3</td>\n    </tr>\n    <tr>\n      <th>1905</th>\n      <td>41.7</td>\n      <td>20.6</td>\n    </tr>\n    <tr>\n      <th>1906</th>\n      <td>19.0</td>\n      <td>18.1</td>\n    </tr>\n    <tr>\n      <th>1907</th>\n      <td>13.0</td>\n      <td>21.4</td>\n    </tr>\n    <tr>\n      <th>1908</th>\n      <td>8.3</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>1909</th>\n      <td>9.1</td>\n      <td>25.4</td>\n    </tr>\n    <tr>\n      <th>1910</th>\n      <td>7.4</td>\n      <td>27.1</td>\n    </tr>\n    <tr>\n      <th>1911</th>\n      <td>8.0</td>\n      <td>40.3</td>\n    </tr>\n    <tr>\n      <th>1912</th>\n      <td>12.3</td>\n      <td>57.0</td>\n    </tr>\n    <tr>\n      <th>1913</th>\n      <td>19.5</td>\n      <td>76.6</td>\n    </tr>\n    <tr>\n      <th>1914</th>\n      <td>45.7</td>\n      <td>52.3</td>\n    </tr>\n    <tr>\n      <th>1915</th>\n      <td>51.1</td>\n      <td>19.5</td>\n    </tr>\n    <tr>\n      <th>1916</th>\n      <td>29.7</td>\n      <td>11.2</td>\n    </tr>\n    <tr>\n      <th>1917</th>\n      <td>15.8</td>\n      <td>7.6</td>\n    </tr>\n    <tr>\n      <th>1918</th>\n      <td>9.7</td>\n      <td>14.6</td>\n    </tr>\n    <tr>\n      <th>1919</th>\n      <td>10.1</td>\n      <td>16.2</td>\n    </tr>\n    <tr>\n      <th>1920</th>\n      <td>8.6</td>\n      <td>24.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nThe data follow a cyclical pattern in which increases in the population of hares are followed by increases in the lynx populations in the subsequent years.\n\n\n```python\ndf[[\"Hare\", \"Lynx\"]].plot(figsize=(10,6), grid=True, xticks = df.index.astype(int)[::2], title=\"Hudson Bay Lynx-Hare Dataset\", ylabel=\"Number of pelts (thousands)\")\n```   \n![](images/lynx_5_1.png){width=600px}\n\n## The Lotka-Volterra model\n\nThe Lotka-Volterra population model the change of predator and pray populations over time:\n - $u(t)\\ge0$ is the population size of the prey species at time t (the hare in our case), and\n - $v(t)\\ge0$ is the population size of the predator species (the lynx in our case).\n\nBy using such a model, some assumptions are made on the dynamics of these populations. We will not go into those here, but they can be found on the [Wikipedia page](https://en.wikipedia.org/wiki/Lotka–Volterra_equations).  \nThe population sizes over times of the two species are modelled in terms of four parameters, $\\alpha,\\beta,\\gamma,\\delta\\ge0$ as\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} u = \\alpha u - \\beta u v\n$$\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} v = \\delta uv - \\gamma v\n$$\n\nIt is these four parameters we will estimate in this post. They have the following interpretations:\n - $\\alpha$ is the growth rate of the prey population,\n - $\\beta$ is the rate of shrinkage relative to the product of the population sizes,\n - $\\gamma$ is the shrinkage rate of the predator population,\n - $\\delta$ is the growth rate of the predator population as a factor of the product of the population sizes.\n\n\nIn the Stan article, the author obtains posterior mean point estimates\n\n$$\n\\hat{\\alpha} = 0.55,\\quad\n\\hat{\\beta} = 0.028,\\quad\n\\hat{\\gamma} = 0.80,\\quad\n\\hat{\\delta} = 0.024,\n$$\nand in the Mathematical Modeling course, the author obtains\n$$\n\\alpha^* = 0.55,\\quad\n\\beta^* = 0.028,\\quad\n\\gamma^* = 0.84,\\quad\n\\delta^* = 0.026.\n$$\n\n\nTo begin, we first set up some plotting functionality for keeping track of the training progress.\n\n\n```python\ndef plot_result(i, t, y, yh, loss, xp=None, lossp=None, params=None):\n    plt.figure(figsize=(8, 4))\n    plt.xlim(0, 40)\n    plt.ylim(0, 85)\n\n    # Data\n    plt.plot(t, y, \"o-\", linewidth=3, alpha=0.2, label=[\"Exact hare\", \"Exact lynx\"])\n    plt.gca().set_prop_cycle(None)  # reset the colors\n    plt.plot(t, yh, linewidth=1, label=[\"Prediction hare\", \"Prediction lynx\"])\n\n    # Physics training ticks\n    if xp is not None:\n        plt.scatter(xp, 0*torch.ones_like(xp), s=3, color=\"tab:green\", alpha=1, marker=\"|\", label=\"Physics loss timesteps\")\n\n    # Legend\n    l = plt.legend(loc=(0.56, 0.1), frameon=False, fontsize=\"large\")\n    plt.setp(l.get_texts(), color=\"k\")\n\n    # Title\n    plt.text(23, 73, f\"Training step: {i}\", fontsize=\"xx-large\", color=\"k\")\n    rmse_text = f\"RMSE data: {int(torch.round(loss))}\"\n    if lossp is not None:\n        rmse_text += f\", physics: {int(torch.round(lossp))}\"\n    plt.text(23, 66, rmse_text, fontsize=\"medium\", color=\"k\")\n    if params is not None:\n        plt.text(23, 60, params, fontsize=\"medium\", color=\"k\")\n\n    plt.axis(\"off\")\n    plt.show()\n```\n\nThe inputs for the model are the timesteps $t=0..20$ corresponding to the years 1900-1920 and the outputs are the numbers of hares and lynxes.\n\n\n```python\ndf[\"Timestep\"] = df.index - min(df.index)\nt = torch.tensor(df[\"Timestep\"].values, dtype=torch.float32).view(-1,1)\ny = torch.tensor(df[[\"Hare\", \"Lynx\"]].values, dtype=torch.float32)\n```\n\nFirst, we define a simple dense feed-forward neural network using Torch.\n\n\n```python\nclass NN(nn.Module):\n    def __init__(self, n_input, n_output, n_hidden, n_layers):\n        super().__init__()\n        self.encode = nn.Sequential(nn.Linear(n_input, n_hidden), nn.Tanh())\n        self.hidden = nn.Sequential(*[nn.Sequential(nn.Linear(n_hidden, n_hidden), nn.Tanh()) for _ in range(n_layers-1)])\n        self.decode = nn.Linear(n_hidden, n_output)\n        \n    def forward(self, x):\n        x = self.encode(x)\n        x = self.hidden(x)\n        x = self.decode(x)\n        return x\n```\n\nWe can train neural network on the dataset (predicting the number of lynx and hares based on timestep $t$) and see that it is complex enough to be able to fit the dataset well. This takes a couple of seconds on my machine.\n\n\n```python\ntorch.manual_seed(123)\nmodel = NN(1, 2, 8, 2)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nfor i in range(50000):\n    optimizer.zero_grad()\n\n    yh = model(t)\n    loss = torch.mean((yh - y)**2)\n\n    loss.backward()\n    optimizer.step()\n    if i%10000 == 0:\n        yh = model(t).detach()\n        \n        plot_result(i, t, y, yh, loss, None)\n```\n\n\n    \n![](images/lynx_14_0.png)\n    \n\n\n\n    \n![](images/lynx_14_1.png)\n    \n\n\n\n    \n![](images/lynx_14_2.png)\n    \n\n\n\n    \n![](images/lynx_14_3.png)\n    \n\n\n\n    \n![](images/lynx_14_4.png)\n    \n\n\nNext, we use exactly the same architecture but add parameters `alpha`, `beta`, `gamma` and `delta` reflecting the parameters in the Lotka-Volterra equations. We initialize them randomly between 0 and 1, and add them to the list of model paramters so that they are updated in the backpropagation steps.  \n\n\n```python\ntorch.manual_seed(123)\nmodel = NN(1, 2, 8, 2)\n\nalpha = torch.rand(1, requires_grad=True)\nbeta = torch.rand(1, requires_grad=True)\ngamma = torch.rand(1, requires_grad=True)\ndelta = torch.rand(1, requires_grad=True)\n\nextra_parameters = [alpha, beta, gamma, delta]\nparameters = list(model.parameters()) + extra_parameters\noptimizer = torch.optim.Adam(parameters, lr=1e-3)\n```\n\nWe define `t_physics`; the range of timesteps on which we numerically evaluate the differential equations. In this case, we evaluate them every 0.1 year.\n\n\n```python\nt_physics = torch.arange(0, len(t), 0.1, dtype=torch.float32).view(-1,1).requires_grad_(True)\n```\n\nTraining the PINN is identical to training the normal neural network, except that the loss function consists of a combination of the data loss and the physics loss.  \nThe data loss is identical to the loss of the previous neural network.  \nFor the physics loss, note that we can rewrite the Lotka-Volterra equations as\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} u - (\\alpha u - \\beta u v) = 0,\n$$\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} v - (\\delta uv - \\gamma v) = 0.\n$$\nWe calculate the mean of the square of the left-hand side across the values of $t$ in `t_physics`, thus punishing the model if the equations are far away from 0. We use `torch.autograd` to estimate the partial derivatives, which means that these partial derivatives are based on what the neural network has learned so far.  \nWe calculate a weighted sum of the physics and data loss, using a weight of `lambda1` set to 0.01. The value of this weight can matter quite a lot and depends on the relative sizes of the errors. Intuitively, it captures how much you trust the data versus the physical laws.\n\n\n```python\nlambda1 = 1e-2\nfor i in range(50000):\n    optimizer.zero_grad()\n\n    # Data loss\n    yh = model(t)\n    data_loss = torch.mean((yh - y)**2)\n\n    # Physics loss\n    yhp = model(t_physics)  # output of the model at the t_physics timesteps\n    u, v = yhp[:, 0], yhp[:, 1]  # hare and lynx populations according to the neural network\n    dudt  = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0].flatten() # time derivative of hare\n    dvdt = torch.autograd.grad(v,  t_physics, torch.ones_like(v),  create_graph=True)[0].flatten() # time derivative of lynx\n    dudt_loss = torch.mean((dudt - (alpha*u - beta*u*v))**2)\n    dvdt_loss = torch.mean((dvdt - (delta*u*v - gamma*v))**2)\n    physics_loss = dudt_loss + dvdt_loss\n\n    loss = data_loss + lambda1*physics_loss\n    loss.backward()\n    optimizer.step()\n\n    if i%10000 == 0:\n        yh = model(t).detach()\n        tp = t_physics.detach()\n        \n        a, b, c, d = [round(param.item(), 3) for param in extra_parameters]\n        params = rf\"$\\alpha={a}, \\beta={b}, \\gamma={c}, \\delta={d}$\"\n        plot_result(i, t, y, yh, loss, tp, physics_loss, params)\n```\n\n\n    \n![](images/lynx_20_0.png)\n    \n\n\n    \n![](images/lynx_20_1.png)\n    \n\n\n\n    \n![](images/lynx_20_2.png)\n    \n\n\n\n    \n![](images/lynx_20_3.png)\n    \n\n\n\n    \n![](images/lynx_20_4.png)\n    \n\n\nWe see that the model is still able to fit well (albeit a little bit worse), and is able to estimate values for the parameters. Specifically, it estimates\n\n$$\n\\hat{\\alpha} = 0.57,\\quad\n\\hat{\\beta} = 0.027,\\quad\n\\hat{\\gamma} = 0.94,\\quad\n\\hat{\\delta} = 0.026,\n$$\nwhich are very close to the results of the other two approaches mentioned above. Only the estimate for $\\gamma$ deviates significantly from the other estimates.  \n\nHowever, this approach is very simple to implement and requires no knowledge about the differential equations or Bayesian modelling software.  \n\nThanks for reading!\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.33","theme":["cosmo","brand"],"title":"Physics-informed neural network for population dynamics","author":"Bas","date":"2024-02-09","math":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}