{"title":"Learning image representation with Keras in R","markdown":{"yaml":{"title":"Learning image representation with Keras in R","author":"Bas","date":"2018-09-20"},"headingText":"Image from Pixabay","containsRefs":false,"markdown":"\n\nAn image can be viewed as a function mapping pixel locations $x,y$ to color values $R,G,B$. Since neural networks are function approximators, we can train such a network to approximate an image. The network is then a representation of the image as a function and its contents can be displayed by evaluating the network for all pixel pairs $x,y$.\n\nThe `Keras` package for `R` is now approximately [1 year old](https://blog.rstudio.com/2017/09/05/keras-for-r/) but I have to admit that I usually go to Python to implement neural networks. Since I wanted to try the `imager` package for `R` for a while, let's hit two birds with one stone and do this exercise in `R`.\n\nFirst, we need the right packages.\n\n``` r\nlibrary(tidyverse)\nlibrary(keras)\nlibrary(imager)\n```\n\nTo honour the name of this blog, let's pick an image of the bird species godwit (scientific name: _limosa_) and load it using `imager`'s `load.image`.\n\n``` r\nim_url <- \"https://cdn.pixabay.com/photo/2015/09/18/00/13/bar-tailed-godwit-944883_640.jpg\"\nim <- load.image(im_url)\ndim(im)\n```\n```\n## [1] 640 480   1   3\n```\n\nThe image is 640x480 pixels in size, consists of 1 frame (it is not a video) and 3 color channels.\nPlotting the image is easy. We choose to show the image without axes and make the margins small for aestetic reasons.\n\n``` r\nplot(im, axes = FALSE)\n```\n![A godwit. Source: pixabay.com](/post/2018-09-20-image-approximation-with-keras-in-r/image.png){width=600px}\n\nWe convert the image to a data frame by calling `as.data.frame` on the image object. This data frame by default has 4 columns: an _x_ and _y_ column to identify each pixel's location, a color channel column _cc_, and the _value_ this channel takes at this pixel. We spread the key-value pairs in the _cc_ and _value_ columns into columns _cc_1_, _cc_2_, _cc_3_ representing the three color values.\n\n``` r\ndf <- im %>% as.data.frame %>% spread(cc, value)\nhead(df)\n```\n```\n##   x y         1         2         3\n## 1 1 1 0.8117647 0.8941176 0.9607843\n## 2 1 2 0.8117647 0.8941176 0.9607843\n## 3 1 3 0.8117647 0.8941176 0.9607843\n## 4 1 4 0.8117647 0.8980392 0.9529412\n## 5 1 5 0.8196078 0.8941176 0.9529412\n## 6 1 6 0.8117647 0.8862745 0.9450980\n```\n\nThe color values are between 0 (absent) and 1 (present).\nWe create input and output matrices which can be fed into keras.\n\n``` r\nX <- as.matrix(select(df, x, y))\nY <- as.matrix(select(df, -x, -y))\n```\n\nCreating a neural network with keras is very easy. We tell it that we want an 'ordinary' feed forward neural network with `keras_model_sequential()`. For starters, we create a one-layer network with 2 input nodes (_x_ and _y_ coordinates) and 3 output nodes (_R_, _G_, _B_ values). We choose a 'sigmoid' activation function because we want the output values to be between 0 and 1.\n\n``` r\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(3, input_shape = 2, activation = \"sigmoid\")\nsummary(model)\n```\n```\n## ___________________________________________________________________________\n## Layer (type)                     Output Shape                  Param #\n## ===========================================================================\n## dense_1 (Dense)                  (None, 3)                     9\n## ===========================================================================\n## Total params: 9\n## Trainable params: 9\n## Non-trainable params: 0\n## ___________________________________________________________________________\n```\n\nWhen compiling the model, we tell it what loss function we need, what optimizer and what kind of metrics we want it to display while training. We choose a _mean_squared_error_ loss, which means that we penalize every color channel equally, taking the square of the error for every channel and every pixel. We optimize the network using the widely used Adam optimizer and we want to see its accuracy while training.\n\n``` r\nmodel %>% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n```\n\nNext, we actually fit the model, giving it the expected input and output matrices _X_ and _Y_. We set the batch size to 128 such that training is faster than the default 32. We train the network for 20 epochs.\n\n``` r\nmodel %>% fit(\n  X, Y,\n  batch_size = 128,\n  epochs = 20\n)\n```\n![Training progress](/post/2018-09-20-image-approximation-with-keras-in-r/trainplot.png)\n\nTraining takes about 1 minute, and as can be seen from the training plot, it seems that both the loss and the accuracy have saturated. An accuracy of 0.9 sounds good, but let's take a look at the output image before we get too happy.\nFirst, we need to recreate the original dataframe with the predicted pixel values. For every $(x,y)$ pair in our matrix _X_ we need to make a prediction. We duplicate the original dataframe fill the color channel columns with the network's output values. We then call `gather` on this dataframe to create the _cc_ and _value_ columns necessary for the `imager` library.\n\n``` r\ndf_out <- df\ndf_out[, 3:5] <- model %>% predict(X)\ndf_out <- df_out %>%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\n```\n\nWith the data frame in the right form, we can display the result next to the original image very easily. We convert the data frame to an image with the `as.cimg` function and paste it next to the original image by wrapping them inside a list and calling `imappend` on it, specifying we want them appended on the horizontal axis.\n\n``` r\nas.cimg(df_out) %>% list(im, .) %>%\n  imappend(\"x\") %>% plot(axes = F)\n```\n![The network learned a gradient](/post/2018-09-20-image-approximation-with-keras-in-r/model1.png){width=600px}\n\nThe result is a vertical gradient, which can also be seen when looking at the model weights.\n\n``` r\nmodel$get_weights()\n```\n```\n## [[1]]\n##              [,1]         [,2]         [,3]\n## [1,]  0.000755751  0.001207316  0.001467007\n## [2,] -0.002216082 -0.004577803 -0.006758745\n##\n## [[2]]\n## [1] 1.460977 2.029516 2.619324\n```\n\nThe values corresponding to the _x_ column are positive, while the _y_ values are negative and about one order of magnitute larger.\n\n# More layers\nNow let's try a network with more layers. More specifically, let's add two layers with both 10 nodes followed by sigmoid activations.\n\n``` r\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(10, input_shape = 2, activation = \"sigmoid\") %>%\n  layer_dense(10, activation = \"sigmoid\") %>%\n  layer_dense(3, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n  X, Y,\n  batch_size = 128,\n  epochs = 20\n)\n\ndf_out <- df\ndf_out[, 3:5] <- model %>% predict(X)\ndf_out <- df_out %>%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\n\nas.cimg(df_out) %>% list(im, .) %>%\n  imappend(\"x\") %>% plot(axes = F)\n```\n![Some features of the bird appear](/post/2018-09-20-image-approximation-with-keras-in-r/model2.png){width=600px}\n\nAs can be seen, the image is already picking up some countours of the bird, together with some 'rays' coming from the top-left corner. If we go for an even deeper network, more of the bird's features can be recognized from the image.\n\n``` r\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(100, input_shape = 2, activation = \"tanh\") %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(3) %>%\n  layer_activation_relu(max_value=1)\n\nmodel %>% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n  X, Y,\n  batch_size = 128,\n  epochs = 100\n)\n\ndf_out <- df\ndf_out[, 3:5] <- model %>% predict(X)\ndf_out <- df_out %>%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\n\nas.cimg(df_out) %>% list(im, .) %>%\n  imappend(\"x\") %>% plot(axes=F)\n```\n![The image is nicely recovered](/post/2018-09-20-image-approximation-with-keras-in-r/model3.png){width=600px}\n\nWays to improve this might include more layers, more epochs, different activation functions, different loss function. For now, I'm really happy with the result!\n","srcMarkdownNoYaml":"\n\nAn image can be viewed as a function mapping pixel locations $x,y$ to color values $R,G,B$. Since neural networks are function approximators, we can train such a network to approximate an image. The network is then a representation of the image as a function and its contents can be displayed by evaluating the network for all pixel pairs $x,y$.\n\nThe `Keras` package for `R` is now approximately [1 year old](https://blog.rstudio.com/2017/09/05/keras-for-r/) but I have to admit that I usually go to Python to implement neural networks. Since I wanted to try the `imager` package for `R` for a while, let's hit two birds with one stone and do this exercise in `R`.\n\nFirst, we need the right packages.\n\n``` r\nlibrary(tidyverse)\nlibrary(keras)\nlibrary(imager)\n```\n\nTo honour the name of this blog, let's pick an image of the bird species godwit (scientific name: _limosa_) and load it using `imager`'s `load.image`.\n\n``` r\n# Image from Pixabay\nim_url <- \"https://cdn.pixabay.com/photo/2015/09/18/00/13/bar-tailed-godwit-944883_640.jpg\"\nim <- load.image(im_url)\ndim(im)\n```\n```\n## [1] 640 480   1   3\n```\n\nThe image is 640x480 pixels in size, consists of 1 frame (it is not a video) and 3 color channels.\nPlotting the image is easy. We choose to show the image without axes and make the margins small for aestetic reasons.\n\n``` r\nplot(im, axes = FALSE)\n```\n![A godwit. Source: pixabay.com](/post/2018-09-20-image-approximation-with-keras-in-r/image.png){width=600px}\n\nWe convert the image to a data frame by calling `as.data.frame` on the image object. This data frame by default has 4 columns: an _x_ and _y_ column to identify each pixel's location, a color channel column _cc_, and the _value_ this channel takes at this pixel. We spread the key-value pairs in the _cc_ and _value_ columns into columns _cc_1_, _cc_2_, _cc_3_ representing the three color values.\n\n``` r\ndf <- im %>% as.data.frame %>% spread(cc, value)\nhead(df)\n```\n```\n##   x y         1         2         3\n## 1 1 1 0.8117647 0.8941176 0.9607843\n## 2 1 2 0.8117647 0.8941176 0.9607843\n## 3 1 3 0.8117647 0.8941176 0.9607843\n## 4 1 4 0.8117647 0.8980392 0.9529412\n## 5 1 5 0.8196078 0.8941176 0.9529412\n## 6 1 6 0.8117647 0.8862745 0.9450980\n```\n\nThe color values are between 0 (absent) and 1 (present).\nWe create input and output matrices which can be fed into keras.\n\n``` r\nX <- as.matrix(select(df, x, y))\nY <- as.matrix(select(df, -x, -y))\n```\n\nCreating a neural network with keras is very easy. We tell it that we want an 'ordinary' feed forward neural network with `keras_model_sequential()`. For starters, we create a one-layer network with 2 input nodes (_x_ and _y_ coordinates) and 3 output nodes (_R_, _G_, _B_ values). We choose a 'sigmoid' activation function because we want the output values to be between 0 and 1.\n\n``` r\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(3, input_shape = 2, activation = \"sigmoid\")\nsummary(model)\n```\n```\n## ___________________________________________________________________________\n## Layer (type)                     Output Shape                  Param #\n## ===========================================================================\n## dense_1 (Dense)                  (None, 3)                     9\n## ===========================================================================\n## Total params: 9\n## Trainable params: 9\n## Non-trainable params: 0\n## ___________________________________________________________________________\n```\n\nWhen compiling the model, we tell it what loss function we need, what optimizer and what kind of metrics we want it to display while training. We choose a _mean_squared_error_ loss, which means that we penalize every color channel equally, taking the square of the error for every channel and every pixel. We optimize the network using the widely used Adam optimizer and we want to see its accuracy while training.\n\n``` r\nmodel %>% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n```\n\nNext, we actually fit the model, giving it the expected input and output matrices _X_ and _Y_. We set the batch size to 128 such that training is faster than the default 32. We train the network for 20 epochs.\n\n``` r\nmodel %>% fit(\n  X, Y,\n  batch_size = 128,\n  epochs = 20\n)\n```\n![Training progress](/post/2018-09-20-image-approximation-with-keras-in-r/trainplot.png)\n\nTraining takes about 1 minute, and as can be seen from the training plot, it seems that both the loss and the accuracy have saturated. An accuracy of 0.9 sounds good, but let's take a look at the output image before we get too happy.\nFirst, we need to recreate the original dataframe with the predicted pixel values. For every $(x,y)$ pair in our matrix _X_ we need to make a prediction. We duplicate the original dataframe fill the color channel columns with the network's output values. We then call `gather` on this dataframe to create the _cc_ and _value_ columns necessary for the `imager` library.\n\n``` r\ndf_out <- df\ndf_out[, 3:5] <- model %>% predict(X)\ndf_out <- df_out %>%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\n```\n\nWith the data frame in the right form, we can display the result next to the original image very easily. We convert the data frame to an image with the `as.cimg` function and paste it next to the original image by wrapping them inside a list and calling `imappend` on it, specifying we want them appended on the horizontal axis.\n\n``` r\nas.cimg(df_out) %>% list(im, .) %>%\n  imappend(\"x\") %>% plot(axes = F)\n```\n![The network learned a gradient](/post/2018-09-20-image-approximation-with-keras-in-r/model1.png){width=600px}\n\nThe result is a vertical gradient, which can also be seen when looking at the model weights.\n\n``` r\nmodel$get_weights()\n```\n```\n## [[1]]\n##              [,1]         [,2]         [,3]\n## [1,]  0.000755751  0.001207316  0.001467007\n## [2,] -0.002216082 -0.004577803 -0.006758745\n##\n## [[2]]\n## [1] 1.460977 2.029516 2.619324\n```\n\nThe values corresponding to the _x_ column are positive, while the _y_ values are negative and about one order of magnitute larger.\n\n# More layers\nNow let's try a network with more layers. More specifically, let's add two layers with both 10 nodes followed by sigmoid activations.\n\n``` r\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(10, input_shape = 2, activation = \"sigmoid\") %>%\n  layer_dense(10, activation = \"sigmoid\") %>%\n  layer_dense(3, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n  X, Y,\n  batch_size = 128,\n  epochs = 20\n)\n\ndf_out <- df\ndf_out[, 3:5] <- model %>% predict(X)\ndf_out <- df_out %>%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\n\nas.cimg(df_out) %>% list(im, .) %>%\n  imappend(\"x\") %>% plot(axes = F)\n```\n![Some features of the bird appear](/post/2018-09-20-image-approximation-with-keras-in-r/model2.png){width=600px}\n\nAs can be seen, the image is already picking up some countours of the bird, together with some 'rays' coming from the top-left corner. If we go for an even deeper network, more of the bird's features can be recognized from the image.\n\n``` r\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(100, input_shape = 2, activation = \"tanh\") %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(100, activation = \"relu\") %>%\n  layer_dense(3) %>%\n  layer_activation_relu(max_value=1)\n\nmodel %>% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n  X, Y,\n  batch_size = 128,\n  epochs = 100\n)\n\ndf_out <- df\ndf_out[, 3:5] <- model %>% predict(X)\ndf_out <- df_out %>%\n  gather(key = \"cc\", value = \"value\", -x, -y, convert = TRUE)\n\nas.cimg(df_out) %>% list(im, .) %>%\n  imappend(\"x\") %>% plot(axes=F)\n```\n![The image is nicely recovered](/post/2018-09-20-image-approximation-with-keras-in-r/model3.png){width=600px}\n\nWays to improve this might include more layers, more epochs, different activation functions, different loss function. For now, I'm really happy with the result!\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"html-math-method":"katex","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.33","theme":{"light":["cosmo"],"dark":["solar"]},"title":"Learning image representation with Keras in R","author":"Bas","date":"2018-09-20"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}