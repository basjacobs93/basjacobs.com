---
title: 'Physics-informed neural network for population dynamics'
author: Bas
date: '2024-02-09'
math: true
---



<p>A Physics-Informed Neural Network (PINN) is a neural network that incorporates knowledge about physical laws alongside data. These physical (or chemical, or biological) laws can be incorporated into the network in the form of differential equations. Among other things, PINNs can be used to estimate parameters of a differential equation based on observational data.</p>
<p>Last week, Dr. Riccardo Taormina gave a guest lecture in my organization in which he explained the workings and advantages of PINNs. In the accompanying workshop he used synthetic data to show that it is possible to estimate the parameters of differential equations of a harmonic oscillator and an advection–diffusion process.</p>
<div class="float">
<img src="https://www2.nau.edu/lrm22/lessons/predator_prey/hare-lynx.jpg" alt="Lynx and snowshoe hare (photo by Tom and Pat Leeson)" />
<div class="figcaption">Lynx and snowshoe hare (photo by Tom and Pat Leeson)</div>
</div>
<p>I once read <a href="https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html">an article by Bob Carpenter</a> about predator-prey population dynamics of hares and lynxes in Canada. The interesting thing of this article is that it uses actual observations rather than synthetical data. The author uses data for the number of pelts of these hares and lynxes obtained by Hudson’s Bay Company in the years 1900-1920 to estimate the parameters of a <a href="https://en.wikipedia.org/wiki/Lotka–Volterra_equations">Lotka-Volterra model</a>. He uses the statistical modeling software <a href="https://mc-stan.org">Stan</a> to generate these estimates based on Markov-Chain Monte-Carlo. This gives posterior estimates (distributions), which elegantly account for measurement and estimation uncertainty.
In the <a href="https://jmahaffy.sdsu.edu/courses/f09/math636/lectures/lotka/qualde2.html">Joseph M. Mahaffy course Mathematical Modeling</a>, the author uses an optimization routine to estimate the parameters of the Lotka-Volterra equations using the same dataset.</p>
<p>In this post, I will show that PINNs can also be used to estimate those parameters. Let’s get started!</p>
<div id="packages" class="section level2">
<h2>Packages</h2>
<p>We will use the Python language with packages Pandas to load the data, Torch to train the neural network and M:w
plotlib for plotting.</p>
<pre class="python"><code>%matplotlib inline
import pandas as pd
import torch
from torch import nn
from matplotlib import pyplot as plt</code></pre>
</div>
<div id="the-data" class="section level2">
<h2>The data</h2>
<p>The data can be downloaded from various places, all derived from <a href="http://www.math.tamu.edu/~phoward/m442/modbasics.pdf" class="uri">http://www.math.tamu.edu/~phoward/m442/modbasics.pdf</a>. I load it from <a href="https://raw.githubusercontent.com/cas-bioinf/statistical-simulations/master/hudson-bay-lynx-hare.csv">here</a>. The dataset consists of observations of lynxes and Hare pelts (in thousands) across the years 1900 to 1920.</p>
<pre class="python"><code>url = &quot;https://raw.githubusercontent.com/cas-bioinf/statistical-simulations/master/hudson-bay-lynx-hare.csv&quot;
df = pd.read_csv(url, sep=&quot;,\\s&quot;, skiprows=2, index_col=0)
df</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Lynx
</th>
<th>
Hare
</th>
</tr>
<tr>
<th>
Year
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1900
</th>
<td>
4.0
</td>
<td>
30.0
</td>
</tr>
<tr>
<th>
1901
</th>
<td>
6.1
</td>
<td>
47.2
</td>
</tr>
<tr>
<th>
1902
</th>
<td>
9.8
</td>
<td>
70.2
</td>
</tr>
<tr>
<th>
1903
</th>
<td>
35.2
</td>
<td>
77.4
</td>
</tr>
<tr>
<th>
1904
</th>
<td>
59.4
</td>
<td>
36.3
</td>
</tr>
<tr>
<th>
1905
</th>
<td>
41.7
</td>
<td>
20.6
</td>
</tr>
<tr>
<th>
1906
</th>
<td>
19.0
</td>
<td>
18.1
</td>
</tr>
<tr>
<th>
1907
</th>
<td>
13.0
</td>
<td>
21.4
</td>
</tr>
<tr>
<th>
1908
</th>
<td>
8.3
</td>
<td>
22.0
</td>
</tr>
<tr>
<th>
1909
</th>
<td>
9.1
</td>
<td>
25.4
</td>
</tr>
<tr>
<th>
1910
</th>
<td>
7.4
</td>
<td>
27.1
</td>
</tr>
<tr>
<th>
1911
</th>
<td>
8.0
</td>
<td>
40.3
</td>
</tr>
<tr>
<th>
1912
</th>
<td>
12.3
</td>
<td>
57.0
</td>
</tr>
<tr>
<th>
1913
</th>
<td>
19.5
</td>
<td>
76.6
</td>
</tr>
<tr>
<th>
1914
</th>
<td>
45.7
</td>
<td>
52.3
</td>
</tr>
<tr>
<th>
1915
</th>
<td>
51.1
</td>
<td>
19.5
</td>
</tr>
<tr>
<th>
1916
</th>
<td>
29.7
</td>
<td>
11.2
</td>
</tr>
<tr>
<th>
1917
</th>
<td>
15.8
</td>
<td>
7.6
</td>
</tr>
<tr>
<th>
1918
</th>
<td>
9.7
</td>
<td>
14.6
</td>
</tr>
<tr>
<th>
1919
</th>
<td>
10.1
</td>
<td>
16.2
</td>
</tr>
<tr>
<th>
1920
</th>
<td>
8.6
</td>
<td>
24.7
</td>
</tr>
</tbody>
</table>
</div>
<p>The data follow a cyclical pattern in which increases in the population of hares are followed by increases in the lynx populations in the subsequent years.</p>
<pre class="python"><code>df[[&quot;Hare&quot;, &quot;Lynx&quot;]].plot(figsize=(10,6), grid=True, xticks = df.index.astype(int)[::2], title=&quot;Hudson Bay Lynx-Hare Dataset&quot;, ylabel=&quot;Number of pelts (thousands)&quot;)</code></pre>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_5_1.png" width="600" /></p>
</div>
<div id="the-lotka-volterra-model" class="section level2">
<h2>The Lotka-Volterra model</h2>
<p>The Lotka-Volterra population model the change of predator and pray populations over time:
- <span class="math inline">\(u(t)\ge0\)</span> is the population size of the prey species at time t (the hare in our case), and
- <span class="math inline">\(v(t)\ge0\)</span> is the population size of the predator species (the lynx in our case).</p>
<p>By using such a model, some assumptions are made on the dynamics of these populations. We will not go into those here, but they can be found on the <a href="https://en.wikipedia.org/wiki/Lotka–Volterra_equations">Wikipedia page</a>.<br />
The population sizes over times of the two species are modelled in terms of four parameters, <span class="math inline">\(\alpha,\beta,\gamma,\delta\ge0\)</span> as</p>
<p><span class="math display">\[
\frac{\mathrm{d}}{\mathrm{d}t} u = \alpha u - \beta u v
\]</span>
<span class="math display">\[
\frac{\mathrm{d}}{\mathrm{d}t} v = \delta uv - \gamma v
\]</span></p>
<p>It is these four parameters we will estimate in this post. They have the following interpretations:
- <span class="math inline">\(\alpha\)</span> is the growth rate of the prey population,
- <span class="math inline">\(\beta\)</span> is the rate of shrinkage relative to the product of the population sizes,
- <span class="math inline">\(\gamma\)</span> is the shrinkage rate of the predator population,
- <span class="math inline">\(\delta\)</span> is the growth rate of the predator population as a factor of the product of the population sizes.</p>
<p>In the Stan article, the author obtains posterior mean point estimates</p>
<p><span class="math display">\[
\hat{\alpha} = 0.55,\quad
\hat{\beta} = 0.028,\quad
\hat{\gamma} = 0.80,\quad
\hat{\delta} = 0.024,
\]</span>
and in the Mathematical Modeling course, the author obtains
<span class="math display">\[
\alpha^* = 0.55,\quad
\beta^* = 0.028,\quad
\gamma^* = 0.84,\quad
\delta^* = 0.026.
\]</span></p>
<p>To begin, we first set up some plotting functionality for keeping track of the training progress.</p>
<pre class="python"><code>def plot_result(i, t, y, yh, loss, xp=None, lossp=None, params=None):
    plt.figure(figsize=(8, 4))
    plt.xlim(0, 40)
    plt.ylim(0, 85)

    # Data
    plt.plot(t, y, &quot;o-&quot;, linewidth=3, alpha=0.2, label=[&quot;Exact hare&quot;, &quot;Exact lynx&quot;])
    plt.gca().set_prop_cycle(None)  # reset the colors
    plt.plot(t, yh, linewidth=1, label=[&quot;Prediction hare&quot;, &quot;Prediction lynx&quot;])

    # Physics training ticks
    if xp is not None:
        plt.scatter(xp, 0*torch.ones_like(xp), s=3, color=&quot;tab:green&quot;, alpha=1, marker=&quot;|&quot;, label=&quot;Physics loss timesteps&quot;)

    # Legend
    l = plt.legend(loc=(0.56, 0.1), frameon=False, fontsize=&quot;large&quot;)
    plt.setp(l.get_texts(), color=&quot;k&quot;)

    # Title
    plt.text(23, 73, f&quot;Training step: {i}&quot;, fontsize=&quot;xx-large&quot;, color=&quot;k&quot;)
    rmse_text = f&quot;RMSE data: {int(torch.round(loss))}&quot;
    if lossp is not None:
        rmse_text += f&quot;, physics: {int(torch.round(lossp))}&quot;
    plt.text(23, 66, rmse_text, fontsize=&quot;medium&quot;, color=&quot;k&quot;)
    if params is not None:
        plt.text(23, 60, params, fontsize=&quot;medium&quot;, color=&quot;k&quot;)

    plt.axis(&quot;off&quot;)
    plt.show()</code></pre>
<p>The inputs for the model are the timesteps <span class="math inline">\(t=0..20\)</span> corresponding to the years 1900-1920 and the outputs are the numbers of hares and lynxes.</p>
<pre class="python"><code>df[&quot;Timestep&quot;] = df.index - min(df.index)
t = torch.tensor(df[&quot;Timestep&quot;].values, dtype=torch.float32).view(-1,1)
y = torch.tensor(df[[&quot;Hare&quot;, &quot;Lynx&quot;]].values, dtype=torch.float32)</code></pre>
<p>First, we define a simple dense feed-forward neural network using Torch.</p>
<pre class="python"><code>class NN(nn.Module):
    def __init__(self, n_input, n_output, n_hidden, n_layers):
        super().__init__()
        self.encode = nn.Sequential(nn.Linear(n_input, n_hidden), nn.Tanh())
        self.hidden = nn.Sequential(*[nn.Sequential(nn.Linear(n_hidden, n_hidden), nn.Tanh()) for _ in range(n_layers-1)])
        self.decode = nn.Linear(n_hidden, n_output)
        
    def forward(self, x):
        x = self.encode(x)
        x = self.hidden(x)
        x = self.decode(x)
        return x</code></pre>
<p>We can train neural network on the dataset (predicting the number of lynx and hares based on timestep <span class="math inline">\(t\)</span>) and see that it is complex enough to be able to fit the dataset well. This takes a couple of seconds on my machine.</p>
<pre class="python"><code>torch.manual_seed(123)
model = NN(1, 2, 8, 2)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
for i in range(50000):
    optimizer.zero_grad()

    yh = model(t)
    loss = torch.mean((yh - y)**2)

    loss.backward()
    optimizer.step()
    if i%10000 == 0:
        yh = model(t).detach()
        
        plot_result(i, t, y, yh, loss, None)</code></pre>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_14_0.png" /></p>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_14_1.png" /></p>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_14_2.png" /></p>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_14_3.png" /></p>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_14_4.png" /></p>
<p>Next, we use exactly the same architecture but add parameters <code>alpha</code>, <code>beta</code>, <code>gamma</code> and <code>delta</code> reflecting the parameters in the Lotka-Volterra equations. We initialize them randomly between 0 and 1, and add them to the list of model paramters so that they are updated in the backpropagation steps.</p>
<pre class="python"><code>torch.manual_seed(123)
model = NN(1, 2, 8, 2)

alpha = torch.rand(1, requires_grad=True)
beta = torch.rand(1, requires_grad=True)
gamma = torch.rand(1, requires_grad=True)
delta = torch.rand(1, requires_grad=True)

extra_parameters = [alpha, beta, gamma, delta]
parameters = list(model.parameters()) + extra_parameters
optimizer = torch.optim.Adam(parameters, lr=1e-3)</code></pre>
<p>We define <code>t_physics</code>; the range of timesteps on which we numerically evaluate the differential equations. In this case, we evaluate them every 0.1 year.</p>
<pre class="python"><code>t_physics = torch.arange(0, len(t), 0.1, dtype=torch.float32).view(-1,1).requires_grad_(True)</code></pre>
<p>Training the PINN is identical to training the normal neural network, except that the loss function consists of a combination of the data loss and the physics loss.<br />
The data loss is identical to the loss of the previous neural network.<br />
For the physics loss, note that we can rewrite the Lotka-Volterra equations as
<span class="math display">\[
\frac{\mathrm{d}}{\mathrm{d}t} u - (\alpha u - \beta u v) = 0,
\]</span>
<span class="math display">\[
\frac{\mathrm{d}}{\mathrm{d}t} v - (\delta uv - \gamma v) = 0.
\]</span>
We calculate the mean of the square of the left-hand side across the values of <span class="math inline">\(t\)</span> in <code>t_physics</code>, thus punishing the model if the equations are far away from 0. We use <code>torch.autograd</code> to estimate the partial derivatives, which means that these partial derivatives are based on what the neural network has learned so far.<br />
We calculate a weighted sum of the physics and data loss, using a weight of <code>lambda1</code> set to 0.01. The value of this weight can matter quite a lot and depends on the relative sizes of the errors. Intuitively, it captures how much you trust the data versus the physical laws.</p>
<pre class="python"><code>lambda1 = 1e-2
for i in range(50000):
    optimizer.zero_grad()

    # Data loss
    yh = model(t)
    data_loss = torch.mean((yh - y)**2)

    # Physics loss
    yhp = model(t_physics)  # output of the model at the t_physics timesteps
    u, v = yhp[:, 0], yhp[:, 1]  # hare and lynx populations according to the neural network
    dudt  = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0].flatten() # time derivative of hare
    dvdt = torch.autograd.grad(v,  t_physics, torch.ones_like(v),  create_graph=True)[0].flatten() # time derivative of lynx
    dudt_loss = torch.mean((dudt - (alpha*u - beta*u*v))**2)
    dvdt_loss = torch.mean((dvdt - (delta*u*v - gamma*v))**2)
    physics_loss = dudt_loss + dvdt_loss

    loss = data_loss + lambda1*physics_loss
    loss.backward()
    optimizer.step()

    if i%10000 == 0:
        yh = model(t).detach()
        tp = t_physics.detach()
        
        a, b, c, d = [round(param.item(), 3) for param in extra_parameters]
        params = rf&quot;$\alpha={a}, \beta={b}, \gamma={c}, \delta={d}$&quot;
        plot_result(i, t, y, yh, loss, tp, physics_loss, params)</code></pre>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_20_0.png" /></p>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_20_1.png" /></p>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_20_2.png" /></p>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_20_3.png" /></p>
<p><img src="/post/2024-02-09-physics-informed-neural-networks-for-population-dynamics/lynx_20_4.png" /></p>
<p>We see that the model is still able to fit well (albeit a little bit worse), and is able to estimate values for the parameters. Specifically, it estimates</p>
<p><span class="math display">\[
\hat{\alpha} = 0.57,\quad
\hat{\beta} = 0.027,\quad
\hat{\gamma} = 0.94,\quad
\hat{\delta} = 0.026,
\]</span>
which are very close to the results of the other two approaches mentioned above. Only the estimate for <span class="math inline">\(\gamma\)</span> deviates significantly from the other estimates.</p>
<p>However, this approach is very simple to implement and requires no knowledge about the differential equations or Bayesian modelling software.</p>
<p>Thanks for reading!</p>
</div>
