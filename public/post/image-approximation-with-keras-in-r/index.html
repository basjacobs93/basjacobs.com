<!DOCTYPE html>
<html lang="en-us">
	<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Bas Jacobs">
<meta name="description" content="My blog about math, programming and data science">
<meta name="generator" content="Hugo 0.45.1" />
<title>Learning image representation with Keras in R</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:400,500|Open+Sans:400|Source+Code+Pro:400,700"> 
<link rel="stylesheet" href="/css/style.css">


<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-128252699-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
<body>
    <nav class="main-nav">
  <a href='/'>
  <img id="logo" src="/images/logo.svg" width="50"/>
  </a>
  
	<a href='/'> Posts</a>

	
 		<a href='/about/'>About</a>
  
  
	
		<a href="/index.xml">Subscribe</a>
	
</nav>

    <section id="wrapper">
        
        
<article class="post">
    <header>
        <h1>Learning image representation with Keras in R</h1>
        <h2 class="subtitle"></h2>
        <h2 class="headline">
        2018-20-09
        </h2>
    </header>
    <section id="post-body">
        <script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>An image can be viewed as a function mapping pixel locations <span class="math inline">\(x,y\)</span> to color values <span class="math inline">\(R,G,B\)</span>. Since neural networks are function approximators, we can train such a network to approximate an image. The network is then a representation of the image as a function and its contents can be displayed by evaluating the network for all pixel pairs <span class="math inline">\(x,y\)</span>.</p>
<p>The <code>Keras</code> package for <code>R</code> is now approximately <a href="https://blog.rstudio.com/2017/09/05/keras-for-r/">1 year old</a> but I have to admit that I usually go to Python to implement neural networks. Since I wanted to try the <code>imager</code> package for <code>R</code> for a while, let’s hit two birds with one stone and do this exercise in <code>R</code>.</p>
<p>First, we need the right packages.</p>
<pre class="r"><code>library(tidyverse)
library(keras)
library(imager)</code></pre>
<p>To honour the name of this blog, let’s pick an image of the bird species godwit (scientific name: <em>limosa</em>) and load it using <code>imager</code>’s <code>load.image</code>.</p>
<pre class="r"><code># Image from Pixabay
im_url &lt;- &quot;https://cdn.pixabay.com/photo/2015/09/18/00/13/bar-tailed-godwit-944883_640.jpg&quot;
im &lt;- load.image(im_url)
dim(im)</code></pre>
<pre><code>## [1] 640 480   1   3</code></pre>
<p>The image is 640x480 pixels in size, consists of 1 frame (it is not a video) and 3 color channels.<br />
Plotting the image is easy. We choose to show the image without axes and make the margins small for aestetic reasons.</p>
<pre class="r"><code>plot(im, axes = FALSE)</code></pre>
<div class="figure">
<img src="/post/2018-09-20-image-approximation-with-keras-in-r_files/image.png" width="600" alt="" />
<p class="caption">A godwit. Source: pixabay.com</p>
</div>
<p>We convert the image to a data frame by calling <code>as.data.frame</code> on the image object. This data frame by default has 4 columns: an <em>x</em> and <em>y</em> column to identify each pixel’s location, a color channel column <em>cc</em>, and the <em>value</em> this channel takes at this pixel. We spread the key-value pairs in the <em>cc</em> and <em>value</em> columns into columns <em>cc_1</em>, <em>cc_2</em>, <em>cc_3</em> representing the three color values.</p>
<pre class="r"><code>df &lt;- im %&gt;% as.data.frame %&gt;% spread(cc, value)
head(df)</code></pre>
<pre><code>##   x y         1         2         3
## 1 1 1 0.8117647 0.8941176 0.9607843
## 2 1 2 0.8117647 0.8941176 0.9607843
## 3 1 3 0.8117647 0.8941176 0.9607843
## 4 1 4 0.8117647 0.8980392 0.9529412
## 5 1 5 0.8196078 0.8941176 0.9529412
## 6 1 6 0.8117647 0.8862745 0.9450980</code></pre>
<p>The color values are between 0 (absent) and 1 (present).<br />
We create input and output matrices which can be fed into keras.</p>
<pre class="r"><code>X &lt;- as.matrix(select(df, x, y))
Y &lt;- as.matrix(select(df, -x, -y))</code></pre>
<p>Creating a neural network with keras is very easy. We tell it that we want an ‘ordinary’ feed forward neural network with <code>keras_model_sequential()</code>. For starters, we create a one-layer network with 2 input nodes (<em>x</em> and <em>y</em> coordinates) and 3 output nodes (<em>R</em>, <em>G</em>, <em>B</em> values). We choose a ‘sigmoid’ activation function because we want the output values to be between 0 and 1.</p>
<pre class="r"><code>model &lt;- keras_model_sequential() 
model %&gt;%
  layer_dense(3, input_shape = 2, activation = &quot;sigmoid&quot;)
summary(model)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 3)                     9           
## ===========================================================================
## Total params: 9
## Trainable params: 9
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p>When compiling the model, we tell it what loss function we need, what optimizer and what kind of metrics we want it to display while training. We choose a <em>mean_squared_error</em> loss, which means that we penalize every color channel equally, taking the square of the error for every channel and every pixel. We optimize the network using the widely used Adam optimizer and we want to see its accuracy while training.</p>
<pre class="r"><code>model %&gt;% compile(
  loss = &quot;mean_squared_error&quot;,
  optimizer = optimizer_adam(),
  metrics = &quot;accuracy&quot;
)</code></pre>
<p>Next, we actually fit the model, giving it the expected input and output matrices <em>X</em> and <em>Y</em>. We set the batch size to 128 such that training is faster than the default 32. We train the network for 20 epochs.</p>
<pre class="r"><code>model %&gt;% fit(
  X, Y, 
  batch_size = 128,
  epochs = 20
)</code></pre>
<div class="figure">
<img src="/post/2018-09-20-image-approximation-with-keras-in-r_files/trainplot.png" alt="" />
<p class="caption">Training progress</p>
</div>
<p>Training takes about 1 minute, and as can be seen from the training plot, it seems that both the loss and the accuracy have saturated. An accuracy of 0.9 sounds good, but let’s take a look at the output image before we get too happy.<br />
First, we need to recreate the original dataframe with the predicted pixel values. For every <span class="math inline">\((x,y)\)</span> pair in our matrix <em>X</em> we need to make a prediction. We duplicate the original dataframe fill the color channel columns with the network’s output values. We then call <code>gather</code> on this dataframe to create the <em>cc</em> and <em>value</em> columns necessary for the <code>imager</code> library.</p>
<pre class="r"><code>df_out &lt;- df
df_out[, 3:5] &lt;- model %&gt;% predict(X)
df_out &lt;- df_out %&gt;%
  gather(key = &quot;cc&quot;, value = &quot;value&quot;, -x, -y, convert = TRUE)</code></pre>
<p>With the data frame in the right form, we can display the result next to the original image very easily. We convert the data frame to an image with the <code>as.cimg</code> function and paste it next to the original image by wrapping them inside a list and calling <code>imappend</code> on it, specifying we want them appended on the horizontal axis.</p>
<pre class="r"><code>as.cimg(df_out) %&gt;% list(im, .) %&gt;%
  imappend(&quot;x&quot;) %&gt;% plot(axes = F)</code></pre>
<div class="figure">
<img src="/post/2018-09-20-image-approximation-with-keras-in-r_files/model1.png" width="600" alt="" />
<p class="caption">The network learned a gradient</p>
</div>
<p>The result is a vertical gradient, which can also be seen when looking at the model weights.</p>
<pre class="r"><code>model$get_weights()</code></pre>
<pre><code>## [[1]]
##              [,1]         [,2]         [,3]
## [1,]  0.000755751  0.001207316  0.001467007
## [2,] -0.002216082 -0.004577803 -0.006758745
## 
## [[2]]
## [1] 1.460977 2.029516 2.619324</code></pre>
<p>The values corresponding to the <em>x</em> column are positive, while the <em>y</em> values are negative and about one order of magnitute larger.</p>
<div id="more-layers" class="section level1">
<h1>More layers</h1>
<p>Now let’s try a network with more layers. More specifically, let’s add two layers with both 10 nodes followed by sigmoid activations.</p>
<pre class="r"><code>model &lt;- keras_model_sequential()
model %&gt;%
  layer_dense(10, input_shape = 2, activation = &quot;sigmoid&quot;) %&gt;%
  layer_dense(10, activation = &quot;sigmoid&quot;) %&gt;%
  layer_dense(3, activation = &quot;sigmoid&quot;)

model %&gt;% compile(
  loss = &quot;mean_squared_error&quot;,
  optimizer = optimizer_adam(),
  metrics = &quot;accuracy&quot;
)

model %&gt;% fit(
  X, Y,
  batch_size = 128,
  epochs = 20
)

df_out &lt;- df
df_out[, 3:5] &lt;- model %&gt;% predict(X)
df_out &lt;- df_out %&gt;%
  gather(key = &quot;cc&quot;, value = &quot;value&quot;, -x, -y, convert = TRUE)

as.cimg(df_out) %&gt;% list(im, .) %&gt;%
  imappend(&quot;x&quot;) %&gt;% plot(axes = F)</code></pre>
<div class="figure">
<img src="/post/2018-09-20-image-approximation-with-keras-in-r_files/model2.png" width="600" alt="" />
<p class="caption">Some features of the bird appear</p>
</div>
<p>As can be seen, the image is already picking up some countours of the bird, together with some ‘rays’ coming from the top-left corner. If we go for an even deeper network, more of the bird’s features can be recognized from the image.</p>
<pre class="r"><code>model &lt;- keras_model_sequential()
model %&gt;%
  layer_dense(100, input_shape = 2, activation = &quot;tanh&quot;) %&gt;%
  layer_dense(100, activation = &quot;relu&quot;) %&gt;%
  layer_dense(100, activation = &quot;relu&quot;) %&gt;%
  layer_dense(100, activation = &quot;relu&quot;) %&gt;%
  layer_dense(100, activation = &quot;relu&quot;) %&gt;%
  layer_dense(3) %&gt;% 
  layer_activation_relu(max_value=1)

model %&gt;% compile(
  loss = &quot;mean_squared_error&quot;,
  optimizer = optimizer_adam(),
  metrics = &quot;accuracy&quot;
)

model %&gt;% fit(
  X, Y,
  batch_size = 128,
  epochs = 100
)

df_out &lt;- df
df_out[, 3:5] &lt;- model %&gt;% predict(X)
df_out &lt;- df_out %&gt;%
  gather(key = &quot;cc&quot;, value = &quot;value&quot;, -x, -y, convert = TRUE)

as.cimg(df_out) %&gt;% list(im, .) %&gt;%
  imappend(&quot;x&quot;) %&gt;% plot(axes=F)</code></pre>
<div class="figure">
<img src="/post/2018-09-20-image-approximation-with-keras-in-r_files/model3.png" width="600" alt="" />
<p class="caption">The image is nicely recovered</p>
</div>
<p>Ways to improve this might include more layers, more epochs, different activation functions, different loss function. For now, I’m really happy with the result!</p>
</div>

    </section>
</article>



        
        <footer id="footer">

</footer>
    </section>
    <script src="/js/main.js"></script>
<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js" integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js" integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous"></script>
<script>
  renderMathInElement(document.body,
    {
        delimiters: [
              {left: "$", right: "$", display: false},
              {left: "$$", right: "$$", display: true},
              {left: "\\(", right: "\\)", display: false},
              {left: "\\[", right: "\\]", display: true}
              ]
    }
  );

  var inlineMathArray = document.querySelectorAll("script[type='math/tex']");
  for (var i = 0; i < inlineMathArray.length; i++) {
    var inlineMath = inlineMathArray[i];
    var tex = inlineMath.innerText || inlineMath.textContent;
    var replaced = document.createElement("span");
    replaced.innerHTML = katex.renderToString(tex, {displayMode: false});
    inlineMath.parentNode.replaceChild(replaced, inlineMath);
  }

  var displayMathArray = document.querySelectorAll("script[type='math/tex; mode=display']");
  for (var i = 0; i < displayMathArray.length; i++) {
    console.log(i);
    var displayMath = displayMathArray[i];
    var tex = displayMath.innerHTML;
    var replaced = document.createElement("span");
    replaced.innerHTML = katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
    displayMath.parentNode.replaceChild(replaced, displayMath);
  }
</script>


</body>
</html>
